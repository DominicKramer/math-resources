<?xml version="1.0" encoding="UTF-8" ?>

<chapter acro="E" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns="">
<title>Eigenvalues</title>

<introduction>
<p>When we have a square matrix of size $n$, $A$, and we multiply it by a vector $\vect{x}$ from $\complex{n}$ to form the matrix-vector product (<acroref type="definition" acro="MVP" />), the result is another vector in $\complex{n}$.  So we can adopt a functional view of this computation <mdash /> the act of multiplying by a square matrix is a function that converts one vector ($\vect{x}$) into another one ($A\vect{x}$) of the same size.  For some vectors, this seemingly complicated computation is really no more complicated than scalar multiplication.  The vectors vary according to the choice of $A$, so the question is to determine, for an individual choice of $A$, if there are any such vectors, and if so, which ones.  It happens in a variety of situations that these vectors (and the scalars that go along with them) are of special interest.</p>

<p>We will be solving polynomial equations in this chapter, which raises the specter of complex numbers as roots.  This distinct possibility is our main reason for entertaining the complex numbers throughout the course.  You might be moved to revisit <acroref type="section" acro="CNO" /> and <acroref type="section" acro="O" />.</p>

</introduction>

<xi:include href="./section-EE.xml" />
<xi:include href="./section-PEE.xml" />
<xi:include href="./section-SD.xml" />
<annotatedacronyms>
<!-- %%%%%%%%%% -->
<!-- % -->
<!-- %  Annotated Acronyms E -->
<!-- %  Determinants -->
<!-- % -->
<!-- %%%%%%%%%% -->
<annoacro type="theorem" acro="EMRCP">
<p>
Much of what we know about eigenvalues can be traced to analysis of the characteristic polynomial.  When we first defined eigenvalues, you might have wondered if they were scarce, or abundant.  The characteristic polynomial allows us to answer a question like this with a result like <acroref type="theorem" acro="NEM" /> which tells us there are always a few eigenvalues, but never too many.
</p>
</annoacro>
<annoacro type="theorem" acro="EMNS">
<p>
If <acroref type="theorem" acro="EMRCP" /> allows us to learn about eigenvalues through what we know about roots of polynomials, then <acroref type="theorem" acro="EMNS" /> allows us to learn about eigenvectors, and eigenspaces, from what we already know about null spaces.  These two theorems, along with <acroref type="definition" acro="EEM" />, provide the starting points for discerning the properties of eigenvalues and eigenvectors (to say nothing of actually computing them).
</p>
</annoacro>
<annoacro type="theorem" acro="HMRE">
<p>
As we have remarked before, we choose to include all of the complex numbers in our set of allowed scalars, whereas many introductory texts restrict their attention to just the real numbers.  Here is one of the payoffs to this approach.  Begin with a matrix, possibly containing complex entries, and require the matrix to be Hermitian (<acroref type="definition" acro="HM" />).  In the case of only real entries, this boils down to just requiring the matrix to be symmetric (<acroref type="definition" acro="SYM" />).  Generally, the roots of a characteristic polynomial, even with all real coefficients, can have complex numbers as roots.  But for a Hermitian matrix, all of the eigenvalues are real numbers!  When somebody tells you mathematics can be beautiful, this is an example of what they are talking about.
</p>
</annoacro>
<annoacro type="theorem" acro="DC">
<p>
Diagonalizing a matrix, or the question of if a matrix is diagonalizable, could be viewed as one of a handful of central questions in linear algebra.  Here we have an unequivocal answer to the question of <q>if,</q> along with a proof containing a construction for the diagonalization.  So this theorem is of theoretical and computational interest.  This topic will be important again in <acroref type="chapter" acro="R" />.
</p>
</annoacro>
<annoacro type="theorem" acro="DMFE">
<p>
Another unequivocal answer to the question of if a matrix is diagonalizable, with perhaps a simpler condition to test.  The proof also tells us how to construct the necessary set of $n$ linearly independent eigenvectors <mdash /> just round up bases for each eigenspace and join them together.  No need to test the linear independence of the combined set.
</p>
</annoacro>
<!--  End E.tex annotated acronyms -->
</annotatedacronyms>
</chapter>