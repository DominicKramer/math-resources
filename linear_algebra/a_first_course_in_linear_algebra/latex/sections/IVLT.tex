%%%%(c)
%%%%(c)  This file is a portion of the source for the textbook
%%%%(c)
%%%%(c)    A First Course in Linear Algebra
%%%%(c)    Copyright 2004 by Robert A. Beezer
%%%%(c)
%%%%(c)  See the file COPYING.txt for copying conditions
%%%%(c)
%%%%(c)
%%%%%%%%%%%
%%
%%  Section IVLT
%%  Invertible Linear Transformations
%%
%%%%%%%%%%%
%
\begin{introduction}
\begin{para}In this section we will conclude our introduction to linear transformations by bringing together the twin properties of injectivity and surjectivity and consider linear transformations with both of these properties.\end{para}
\end{introduction}
%
\begin{subsect}{IVLT}{Invertible Linear Transformations}
%
\begin{para}One preliminary definition, and then we will have our main definition for this section.\end{para}
%
\begin{definition}{IDLT}{Identity Linear Transformation}{linear transformation!identity}
\begin{para}The \define{identity linear transformation} on the vector space $W$ is defined as
%
\begin{equation*}
\ltdefn{I_W}{W}{W},\quad\quad \lt{I_W}{\vect{w}}=\vect{w}
\end{equation*}
\end{para}
%
\end{definition}
%
\begin{para}Informally, $I_W$ is the ``do-nothing'' function.  You should check that $I_W$ is really a linear transformation, as claimed, and then compute its kernel and range to see that it is both injective and surjective.  All of these facts should be straightforward to verify (\acronymref{exercise}{IVLT.T05}).  With this in hand we can make our main definition.\end{para}
%
%
\begin{definition}{IVLT}{Invertible Linear Transformations}{linear transformation!invertible}
\begin{para}Suppose that $\ltdefn{T}{U}{V}$ is a linear transformation.  If there is a function $\ltdefn{S}{V}{U}$ such that
%
\begin{align*}
\compose{S}{T}&=I_U & \compose{T}{S}&=I_V
\end{align*}
%
then $T$ is \define{invertible}.  In this case, we call $S$ the \define{inverse} of $T$ and write $S=\ltinverse{T}$.\end{para}
\end{definition}
%
\begin{para}Informally, a linear transformation $T$ is invertible if there is a companion linear transformation, $S$, which ``undoes'' the action of $T$.  When the two linear transformations are applied consecutively (composition), in either order, the result is to have no real effect.  It is entirely analogous to squaring a positive number and then taking its (positive) square root.\end{para}
%
\begin{para}Here is an example of a linear transformation that is invertible.  As usual at the beginning of a section, do not be concerned with where $S$ came from, just understand how it illustrates \acronymref{definition}{IVLT}.\end{para}
%
\begin{example}{AIVLT}{An invertible linear transformation}{linear transformation!invertible}
\begin{para}\acronymref{archetype}{V} is the linear transformation
%
\begin{equation*}
\archetypepart{V}{ltdefn}\end{equation*}
\end{para}
%
\begin{para}Define the function $\ltdefn{S}{M_{22}}{P_3}$ defined by
%
\begin{equation*}
\lt{S}{\begin{bmatrix}a&b\\c&d\end{bmatrix}}=(a - c - d)+ (c + d)x +\frac{1}{2}(a - b - c - d)x^2+cx^3
\end{equation*}
\end{para}
%
\begin{para}Then
%
\begin{align*}
\lt{\left(\compose{T}{S}\right)}{\begin{bmatrix}a&b\\c&d\end{bmatrix}}&=
\lt{T}{\lt{S}{\begin{bmatrix}a&b\\c&d\end{bmatrix}}}\\
%
&=\lt{T}{(a - c - d)+ (c + d)x +\frac{1}{2}(a - b - c - d)x^2+cx^3}\\
&=\begin{bmatrix}
(a - c - d)+ (c + d)&(a - c - d)-2(\frac{1}{2}(a - b - c - d))\\c&(c + d)-c
\end{bmatrix}\\
&=\begin{bmatrix}a&b\\c&d\end{bmatrix}\\
&=\lt{I_{M_{22}}}{\begin{bmatrix}a&b\\c&d\end{bmatrix}}
%
\intertext{and}
%
\lt{\left(\compose{S}{T}\right)}{a+bx+cx^2+dx^3}&=
\lt{S}{\lt{T}{a+bx+cx^2+dx^3}}\\
%
&=\lt{S}{\begin{bmatrix}
a+b&a-2c\\d&b-d
\end{bmatrix}}\\
&=((a+b)-d-(b-d))+
(d+(b-d))x\\
&\quad\quad+\left(\frac{1}{2}((a+b)-(a-2c)-d-(b-d))\right)x^2+
(d)x^3\\
&=a+bx+cx^2+dx^3\\
&=\lt{I_{P_3}}{a+bx+cx^2+dx^3}
\end{align*}
\end{para}
%
\begin{para}For now, understand why these computations show that $T$ is invertible, and that $S=T^{-1}$.  Maybe even be amazed by how $S$ works so perfectly in concert with $T$!  We will see later just how to arrive at the correct form of $S$ (when it is possible).\end{para}
%
\end{example}
%
\begin{para}It can be as instructive to study a linear transformation that is not invertible.\end{para}
%
\begin{example}{ANILT}{A non-invertible linear transformation}{linear transformation!not invertible}
\begin{para}Consider the linear transformation $\ltdefn{T}{\complex{3}}{M_{22}}$ defined by
%
\begin{equation*}
\lt{T}{\colvector{a\\b\\c}}=\begin{bmatrix}
a-b&2a+2b+c\\3a+b+c&-2a-6b-2c
\end{bmatrix}
\end{equation*}
\end{para}
%
\begin{para}Suppose we were to search for an inverse function $\ltdefn{S}{M_{22}}{\complex{3}}$.\end{para}
%
\begin{para}First verify that the $2\times 2$ matrix
$A=\begin{bmatrix}
5&3\\8&2
\end{bmatrix}
$
is not in the range of $T$.  This will amount to finding an input to $T$, $\colvector{a\\b\\c}$, such that
%
\begin{align*}
a-b&=5\\
2a+2b+c&=3\\
3a+b+c&=8\\
-2a-6b-2c&=2
\end{align*}
\end{para}
%
\begin{para}As this system of equations is inconsistent, there is no input column vector, and $A\not\in\rng{T}$.  How should we define $\lt{S}{A}$?  Note that
%
\begin{equation*}
\lt{T}{\lt{S}{A}}=\lt{\left(\compose{T}{S}\right)}{A}=
\lt{I_{M_{22}}}{A}=A
\end{equation*}
\end{para}
%
\begin{para}So any definition we would provide for $\lt{S}{A}$ must then be a column vector that $T$ sends to $A$ and we would have $A\in\rng{T}$, contrary to the definition of $T$.  This is enough to see that there is no function $S$ that will allow us to conclude that $T$ is invertible, since we cannot provide a consistent definition for $\lt{S}{A}$ if we assume $T$ is invertible.\end{para}
%
\begin{para}Even though we now know that $T$ is not invertible, let's not leave this example just yet.  Check that
%
\begin{align*}
\lt{T}{\colvector{1\\-2\\4}}&=\begin{bmatrix}3&2\\5&2\end{bmatrix}=B&
\lt{T}{\colvector{0\\-3\\8}}&=\begin{bmatrix}3&2\\5&2\end{bmatrix}=B
\end{align*}
\end{para}
%
\begin{para}How would we define $\lt{S}{B}$?
%
\begin{align*}
%
\lt{S}{B}&=\lt{S}{\lt{T}{\colvector{1\\-2\\4}}}
=\lt{\left(\compose{S}{T}\right)}{\colvector{1\\-2\\4}}
=\lt{I_{\complex{3}}}{\colvector{1\\-2\\4}}=\colvector{1\\-2\\4}
\intertext{or}
\lt{S}{B}&=\lt{S}{\lt{T}{\colvector{0\\-3\\8}}}
=\lt{\left(\compose{S}{T}\right)}{\colvector{0\\-3\\8}}
=\lt{I_{\complex{3}}}{\colvector{0\\-3\\8}}=\colvector{0\\-3\\8}
\end{align*}
\end{para}
%
\begin{para}Which definition should we provide for $\lt{S}{B}$?  Both are necessary.  But then $S$ is not a function.  So we have a second reason to know that there is no function $S$ that will allow us to conclude that $T$ is invertible.  It happens that there are infinitely many column vectors that $S$ would have to take to $B$.  Construct the kernel of $T$,
%
\begin{equation*}
\krn{T}=\spn{\set{\colvector{-1\\-1\\4}}}
\end{equation*}
\end{para}
%
\begin{para}Now choose either of the two inputs used above for $T$ and add to it a scalar multiple of the basis vector for the kernel of $T$.  For example,
%
\begin{equation*}
\vect{x}=\colvector{1\\-2\\4}+(-2)\colvector{-1\\-1\\4}=\colvector{3\\0\\-4}
\end{equation*}
%
then verify that $\lt{T}{\vect{x}}=B$.  Practice creating a few more inputs for $T$ that would be sent to $B$, and see why it is hopeless to think that we could ever provide a reasonable definition for $\lt{S}{B}$!  There is a ``whole subspace's worth'' of values that $\lt{S}{B}$ would have to take on.\end{para}
%
\end{example}
%
\begin{para}In \acronymref{example}{ANILT} you may have noticed that $T$ is not surjective, since the matrix $A$ was not in the range of $T$.  And $T$ is not injective since there are two different input column vectors that $T$ sends to the matrix $B$.  Linear transformations $T$ that are not surjective lead to putative inverse functions $S$ that are undefined on inputs outside of the range of $T$.  Linear transformations $T$ that are not injective lead to putative inverse functions $S$ that are multiply-defined on each of their inputs.  We will formalize these ideas in \acronymref{theorem}{ILTIS}.\end{para}
%
\begin{para}But first notice in \acronymref{definition}{IVLT} that we only require the inverse (when it exists) to be a function.  When it does exist, it too is a linear transformation.\end{para}
%
\begin{theorem}{ILTLT}{Inverse of a Linear Transformation is a Linear Transformation}{linear transformation!inverse}
\begin{para}Suppose that $\ltdefn{T}{U}{V}$ is an invertible linear transformation.  Then the function $\ltdefn{\ltinverse{T}}{V}{U}$ is a linear transformation.\end{para}
\end{theorem}
%
\begin{proof}
\begin{para}We work through verifying \acronymref{definition}{LT} for $\ltinverse{T}$, using the fact that $T$ is a linear transformation to obtain the second equality in each half of the proof.  To this end, suppose $\vect{x},\,\vect{y}\in V$ and $\alpha\in\complex{\null}$.
%
\begin{align*}
%
\lt{\ltinverse{T}}{\vect{x}+\vect{y}}&=
\lt{\ltinverse{T}}{\lt{T}{\lt{\ltinverse{T}}{\vect{x}}}+\lt{T}{\lt{\ltinverse{T}}{\vect{y}}}}
&&\text{\acronymref{definition}{IVLT}}\\
%
&=\lt{\ltinverse{T}}{\lt{T}{\lt{\ltinverse{T}}{\vect{x}}+\lt{\ltinverse{T}}{\vect{y}}}}&&\text{\acronymref{definition}{LT}}\\
%
&=\lt{\ltinverse{T}}{\vect{x}}+\lt{\ltinverse{T}}{\vect{y}}
&&\text{\acronymref{definition}{IVLT}}
%
\end{align*}
\end{para}
%
\begin{para}Now check the second defining property of a linear transformation for $\ltinverse{T}$,
%
\begin{align*}
%
\lt{\ltinverse{T}}{\alpha\vect{x}}&=
\lt{\ltinverse{T}}{\alpha\lt{T}{\lt{\ltinverse{T}}{\vect{x}}}}
&&\text{\acronymref{definition}{IVLT}}\\
%
&=\lt{\ltinverse{T}}{\lt{T}{\alpha\lt{\ltinverse{T}}{\vect{x}}}}
&&\text{\acronymref{definition}{LT}}\\
%
&=\alpha\lt{\ltinverse{T}}{\vect{x}}
&&\text{\acronymref{definition}{IVLT}}
%
\end{align*}
\end{para}
%
\begin{para}So $\ltinverse{T}$ fulfills the requirements of \acronymref{definition}{LT} and is therefore a linear transformation.\end{para}
%
\end{proof}
%
\begin{para}So when $T$ has an inverse, $\ltinverse{T}$ is also a linear transformation.  Furthermore, $\ltinverse{T}$ is an invertible linear transformation and {\em its} inverse is what you might expect.\end{para}
%
%
\begin{theorem}{IILT}{Inverse of an Invertible Linear Transformation}{linear transformation!inverse of inverse}
\begin{para}Suppose that $\ltdefn{T}{U}{V}$ is an invertible linear transformation.  Then $\ltinverse{T}$ is an invertible linear transformation and $\ltinverse{\left(\ltinverse{T}\right)}=T$.\end{para}
\end{theorem}
%
\begin{proof}
\begin{para}Because $T$ is invertible, \acronymref{definition}{IVLT} tells us there is a function $\ltdefn{\ltinverse{T}}{V}{U}$ such that
%
\begin{align*}
\compose{\ltinverse{T}}{T}&=I_U & \compose{T}{\ltinverse{T}}&=I_V
\end{align*}
\end{para}
%
\begin{para}Additionally, \acronymref{theorem}{ILTLT} tells us that $\ltinverse{T}$ is more than just a function, it is a linear transformation.  Now view these two statements as properties of the linear transformation $\ltinverse{T}$.  In light of \acronymref{definition}{IVLT}, they together say that $\ltinverse{T}$ is invertible (let $T$ play the role of $S$ in the statement of the definition).  Furthermore, the inverse of $\ltinverse{T}$ is then $T$, i.e.\ $\ltinverse{\left(\ltinverse{T}\right)}=T$.\end{para}
\end{proof}
%
\sageadvice{IVLT}{Invertible Linear Transformations}{linear transformation!inverse}
%
\end{subsect}
%
\begin{subsect}{IV}{Invertibility}
%
\begin{para}We now know what an inverse linear transformation is, but just which linear transformations have inverses?  Here is a theorem we have been preparing for all chapter long.\end{para}
%
\begin{theorem}{ILTIS}{Invertible Linear Transformations are Injective and Surjective}{linear transformation!invertible, injective and surjective}
\begin{para}Suppose $\ltdefn{T}{U}{V}$ is a linear transformation.  Then $T$ is invertible if and only if $T$ is injective and surjective.\end{para}
\end{theorem}
%
\begin{proof}
\begin{para}($\Rightarrow$)  Since $T$ is presumed invertible, we can employ its inverse, $\ltinverse{T}$ (\acronymref{definition}{IVLT}).  To see that $T$ is injective, suppose $\vect{x},\,\vect{y}\in U$ and assume that $\lt{T}{\vect{x}}=\lt{T}{\vect{y}}$,
%
\begin{align*}
\vect{x}
&=\lt{I_U}{\vect{x}}&&\text{\acronymref{definition}{IDLT}}\\
&=\lt{\left(\compose{\ltinverse{T}}{T}\right)}{\vect{x}}&&\text{\acronymref{definition}{IVLT}}\\
&=\lt{\ltinverse{T}}{\lt{T}{\vect{x}}}&&\text{\acronymref{definition}{LTC}}\\
&=\lt{\ltinverse{T}}{\lt{T}{\vect{y}}}&&\text{\acronymref{definition}{ILT}}\\
&=\lt{\left(\compose{\ltinverse{T}}{T}\right)}{\vect{y}}&&\text{\acronymref{definition}{LTC}}\\
&=\lt{I_U}{\vect{y}}&&\text{\acronymref{definition}{IVLT}}\\
&=\vect{y}&&\text{\acronymref{definition}{IDLT}}
%
\end{align*}
%
So by \acronymref{definition}{ILT} $T$ is injective.\end{para}
%
\begin{para}To check that $T$ is surjective, suppose $\vect{v}\in V$.  Then $\lt{\ltinverse{T}}{\vect{v}}$ is a vector in $U$.  Compute
%
\begin{align*}
%
\lt{T}{\lt{\ltinverse{T}}{\vect{v}}}
&=\lt{\left(\compose{T}{\ltinverse{T}}\right)}{\vect{v}}&&\text{\acronymref{definition}{LTC}}\\
&=\lt{I_V}{\vect{v}}&&\text{\acronymref{definition}{IVLT}}\\
&=\vect{v}&&\text{\acronymref{definition}{IDLT}}
\end{align*}
%
So there is an element from $U$, when used as an input to $T$ (namely $\lt{\ltinverse{T}}{\vect{v}}$) that produces the desired output, $\vect{v}$, and hence $T$ is surjective by \acronymref{definition}{SLT}.\end{para}
%
\begin{para}($\Leftarrow$)  Now assume that $T$ is both injective and surjective.  We will build a function $\ltdefn{S}{V}{U}$ that will establish that $T$ is invertible.  To this end, choose any $\vect{v}\in V$.  Since $T$ is surjective, \acronymref{theorem}{RSLT} says $\rng{T}=V$, so we have $\vect{v}\in\rng{T}$.  \acronymref{theorem}{RPI} says that the pre-image of $\vect{v}$, $\preimage{T}{\vect{v}}$, is nonempty.  So we can choose a vector from the pre-image of $\vect{v}$, say $\vect{u}$.  In other words, there exists $\vect{u}\in\preimage{T}{\vect{v}}$.\end{para}
%
\begin{para}Since $\preimage{T}{\vect{v}}$ is non-empty, \acronymref{theorem}{KPI} then says that
%
\begin{equation*}
\preimage{T}{\vect{v}}=\setparts{\vect{u}+\vect{z}}{\vect{z}\in\krn{T}}
\end{equation*}
\end{para}
%
\begin{para}However, because $T$ is injective, by \acronymref{theorem}{KILT} the kernel is trivial, $\krn{T}=\set{\zerovector}$.  So the pre-image is a set with just one element, $\preimage{T}{\vect{v}}=\set{\vect{u}}$.  Now we can define $S$ by $\lt{S}{\vect{v}}=\vect{u}$.  This is the key to this half of this proof.  Normally the preimage of a vector from the codomain might be an empty set, or an infinite set.  But surjectivity requires that the preimage not be empty, and then injectivity limits the preimage to a singleton.  Since our choice of $\vect{v}$ was arbitrary, we know that every pre-image for $T$ is a set with a single element.  This allows us to construct $S$ as a {\em function}.  Now that it is defined, verifying that it is the inverse of $T$ will be easy.  Here we go.\end{para}
%
\begin{para}Choose $\vect{u}\in U$.  Define $\vect{v}=\lt{T}{\vect{u}}$.  Then $\preimage{T}{\vect{v}}=\set{\vect{u}}$, so that $\lt{S}{\vect{v}}=\vect{u}$ and,
%
\begin{equation*}
\lt{\left(\compose{S}{T}\right)}{\vect{u}}
=\lt{S}{\lt{T}{\vect{u}}}
=\lt{S}{\vect{v}}
=\vect{u}
=\lt{I_U}{\vect{u}}
\end{equation*}
%
and since our choice of $\vect{u}$ was arbitrary we have function equality, $\compose{S}{T}=I_U$.\end{para}
%
\begin{para}Now choose $\vect{v}\in V$.  Define $\vect{u}$ to be the single vector in the set $\preimage{T}{\vect{v}}$, in other words, $\vect{u}=\lt{S}{\vect{v}}$.  Then $\lt{T}{\vect{u}}=\vect{v}$, so
%
\begin{equation*}
\lt{\left(\compose{T}{S}\right)}{\vect{v}}
=\lt{T}{\lt{S}{\vect{v}}}
=\lt{T}{\vect{u}}
=\vect{v}
=\lt{I_V}{\vect{v}}
\end{equation*}
%
%
and since our choice of $\vect{v}$ was arbitrary we have function equality, $\compose{T}{S}=I_V$.\end{para}
%
\end{proof}
%
\begin{para}When a linear transformation is both injective and surjective, the pre-image of any element of the codomain is a set of size one (a ``singleton'').  This fact allowed us to {\em construct} the inverse linear transformation in one half of the proof of \acronymref{theorem}{ILTIS} (see \acronymref{technique}{C}).  We can follow this approach to construct the inverse of a specific linear transformation, as the next example shows.\end{para}
%
\begin{example}{CIVLT}{Computing the Inverse of a Linear Transformations}{invertible linear transformations!computing}
%
\begin{para}Consider the linear transformation  $\ltdefn{T}{S_{22}}{P_2}$ defined by
%
\begin{align*}
\lt{T}{\begin{bmatrix}a&b\\b&c\end{bmatrix}}
&=
\left(a+b+c\right)
+
\left(-a+2c\right)x
+
\left(2a+3b+6c\right)x^2
\end{align*}\end{para}
%
\begin{para}$T$ is invertible, which you are able to verify, perhaps by determining that the kernel of $T$ is trivial and the range of $T$ is all of $P_2$.  This will be easier once we have \acronymref{theorem}{RPNDD}, which appears later in this section.\end{para}
%
\begin{para}By \acronymref{theorem}{ILTIS} we know $\ltinverse{T}$ exists, and it will be critical shortly to realize that $\ltinverse{T}$ is automatically known to be a linear transformation as well (\acronymref{theorem}{ILTLT}).  To determine the complete behavior of $\ltdefn{\ltinverse{T}}{P_2}{S_{22}}$ we can simply determine its action on a basis for the domain, $P_2$.  This is the substance of \acronymref{theorem}{LTDB}, and an excellent example of its application.   Choose any basis of $P_2$, the simpler the better, such as $B=\set{1,\,x,\,x^2}$.  Values of $\ltinverse{T}$ for these three basis elements will be the single elements of their preimages.  In turn, we have
%
\begin{align*}
\preimage{T}{1}:&\\
&&\lt{T}{\begin{bmatrix}a&b\\b&c\end{bmatrix}}
&=1+0x+0x^2\\
%
&&\begin{bmatrix}
 1 & 1 & 1 & 1\\
 -1 & 0 & 2 & 0\\
 2 & 3 & 6 & 0
\end{bmatrix}
&\rref
\begin{bmatrix}
1 & 0 & 0& -6 \\
0 & 1 & 0& 10 \\
0 & 0 & 1& -3
\end{bmatrix}
\\
&\text{(preimage)}&\preimage{T}{1}
&=\set{\begin{bmatrix}-6&10\\10&-3\end{bmatrix}}\\
%
&\text{(function)}&\lt{\ltinverse{T}}{1}
&=
\begin{bmatrix}-6&10\\10&-3\end{bmatrix}\\
%
%
\preimage{T}{x}:&\\
&&\lt{T}{\begin{bmatrix}a&b\\b&c\end{bmatrix}}
&=0+1x+0x^2\\
%
&&\begin{bmatrix}
 1 & 1 & 1 & 0\\
 -1 & 0 & 2 & 1\\
 2 & 3 & 6 & 0
\end{bmatrix}
&\rref
\begin{bmatrix}
1 & 0 & 0& -3 \\
0 & 1 & 0& 4 \\
0 & 0 & 1&  -1
\end{bmatrix}
\\
&\text{(preimage)}&\preimage{T}{x}
&=\set{\begin{bmatrix}-3&4\\4&-1\end{bmatrix}}\\
%
&\text{(function)}&\lt{\ltinverse{T}}{x}
&=
\begin{bmatrix}-3&4\\4&-1\end{bmatrix}\\
%
%
\preimage{T}{x^2}:&\\
&&\lt{T}{\begin{bmatrix}a&b\\b&c\end{bmatrix}}
&=0+0x+1x^2\\
%
&&\begin{bmatrix}
 1 & 1 & 1 & 0\\
 -1 & 0 & 2 & 0\\
 2 & 3 & 6 & 1
\end{bmatrix}
&\rref
\begin{bmatrix}
1 & 0 & 0& 2 \\
0 & 1 & 0& -3 \\
0 & 0 & 1&  1
\end{bmatrix}
\\
&\text{(preimage)}&\preimage{T}{x^2}
&=\set{\begin{bmatrix}2&-3\\-3&1\end{bmatrix}}\\
%
&\text{(function)}&\lt{\ltinverse{T}}{x^2}
&=
\begin{bmatrix}2&-3\\-3&1\end{bmatrix}
\end{align*}
\end{para}
%
\begin{para}\acronymref{theorem}{LTDB} says, informally, ``it is enough to know what a linear transformation does to a basis.''  Formally, we have the outputs of $\ltinverse{T}$ for a basis, so by \acronymref{theorem}{LTDB} there is a unique linear transformation with these outputs.  So we put this information to work.  The key step here is that we can convert any element of $P_2$ into a linear combination of the elements of the basis $B$ (\acronymref{theorem}{VRRB}).  We are after a ``formula'' for the value of $\ltinverse{T}$ on a generic element of $P_2$, say $p+qx+rx^2$.
%
\begin{align*}
\lt{\ltinverse{T}}{p+qx+rx^2}
&=
\lt{\ltinverse{T}}{p(1)+q(x)+r(x^2)}
&&\text{\acronymref{theorem}{VRRB}}\\
&=
p\lt{\ltinverse{T}}{1}+
q\lt{\ltinverse{T}}{x}+
r\lt{\ltinverse{T}}{x^2}
&&\text{\acronymref{theorem}{LTLC}}\\
&=
p\begin{bmatrix}-6&10\\10&-3\end{bmatrix}+
q\begin{bmatrix}-3&4\\4&-1\end{bmatrix}+
r\begin{bmatrix}2&-3\\-3&1\end{bmatrix}\\
&=
\begin{bmatrix}
-6p-3q+2r & 10p+4q-3r \\
10p+4q-3r & -3p -q + r
\end{bmatrix}
\end{align*}
\end{para}
%
\begin{para}Notice how a linear combination in the domain of $\ltinverse{T}$ has been translated into a linear combination in the codomain of $\ltinverse{T}$ since we know $\ltinverse{T}$ is a linear transformation by \acronymref{theorem}{ILTLT}.\end{para}
%
\begin{para}Also, notice how the augmented matrices used to determine the three pre-images could be combined into one calculation of a matrix in extended echelon form, reminiscent of a procedure we know for computing the inverse of a matrix (see \acronymref{example}{CMI}).  Hmmmm.\end{para}
%
\end{example}
%
\begin{para}We will make frequent use of the characterization of invertible linear transformations provided by \acronymref{theorem}{ILTIS}.  The next theorem is a good example of this, and we will use it often, too.\end{para}
%
\begin{theorem}{CIVLT}{Composition of Invertible Linear Transformations}{invertible linear transformations!composition}
\begin{para}Suppose that $\ltdefn{T}{U}{V}$ and $\ltdefn{S}{V}{W}$ are invertible linear transformations.  Then the composition, $\ltdefn{\left(\compose{S}{T}\right)}{U}{W}$ is an invertible linear transformation.\end{para}
\end{theorem}
%
\begin{proof}
\begin{para}Since $S$ and $T$ are both linear transformations,  $\compose{S}{T}$ is also a linear transformation by \acronymref{theorem}{CLTLT}.    Since $S$ and $T$ are both invertible, \acronymref{theorem}{ILTIS} says that $S$ and $T$ are both injective and surjective.  Then \acronymref{theorem}{CILTI} says $\compose{S}{T}$ is injective, and \acronymref{theorem}{CSLTS} says $\compose{S}{T}$ is surjective.  Now apply the ``other half'' of \acronymref{theorem}{ILTIS} and conclude that $\compose{S}{T}$ is invertible.\end{para}
\end{proof}
%
\begin{para}When a composition is invertible, the inverse is easy to construct.\end{para}
%
\begin{theorem}{ICLT}{Inverse of a Composition of Linear Transformations}{inverse!composition of linear transformations}
\begin{para}Suppose that $\ltdefn{T}{U}{V}$ and $\ltdefn{S}{V}{W}$ are invertible linear transformations. Then $\compose{S}{T}$ is invertible and $\ltinverse{\left(\compose{S}{T}\right)}=\compose{\ltinverse{T}}{\ltinverse{S}}$.\end{para}
\end{theorem}
%
\begin{proof}
\begin{para}Compute, for all $\vect{w}\in W$
%
\begin{align*}
\lt{\left(\compose{\left(\compose{S}{T}\right)}{\left(\compose{\ltinverse{T}}{\ltinverse{S}}\right)}\right)}{\vect{w}}
&=\lt{S}{\lt{T}{\lt{\ltinverse{T}}{\lt{\ltinverse{S}}{\vect{w}}}}}\\
&=\lt{S}{\lt{I_V}{\lt{\ltinverse{S}}{\vect{w}}}}&&\text{\acronymref{definition}{IVLT}}\\
&=\lt{S}{\lt{\ltinverse{S}}{\vect{w}}}&&\text{\acronymref{definition}{IDLT}}\\
&=\vect{w}&&\text{\acronymref{definition}{IVLT}}\\
&=\lt{I_W}{\vect{w}}&&\text{\acronymref{definition}{IDLT}}
\end{align*}
\end{para}
%
\begin{para}So $\compose{\left(\compose{S}{T}\right)}{\left(\compose{\ltinverse{T}}{\ltinverse{S}}\right)}=I_W$, and also
%
\begin{align*}
\lt{\left(\compose{\left(\compose{\ltinverse{T}}{\ltinverse{S}}\right)}{\left(\compose{S}{T}\right)}\right)}{\vect{u}}
&=\lt{\ltinverse{T}}{\lt{\ltinverse{S}}{\lt{S}{\lt{T}{\vect{u}}}}}\\
&=\lt{\ltinverse{T}}{\lt{I_V}{\lt{T}{\vect{u}}}}&&\text{\acronymref{definition}{IVLT}}\\
&=\lt{\ltinverse{T}}{\lt{T}{\vect{u}}}&&\text{\acronymref{definition}{IDLT}}\\
&=\vect{u}&&\text{\acronymref{definition}{IVLT}}\\
&=\lt{I_U}{\vect{u}}&&\text{\acronymref{definition}{IDLT}}
\end{align*}
%
so $\compose{\left(\compose{\ltinverse{T}}{\ltinverse{S}}\right)}{\left(\compose{S}{T}\right)}=I_U$.\end{para}
%
\begin{para}By \acronymref{definition}{IVLT}, $\compose{S}{T}$ is invertible and $\ltinverse{\left(\compose{S}{T}\right)}=\compose{\ltinverse{T}}{\ltinverse{S}}$.\end{para}
%
\end{proof}
%
\begin{para}Notice that this theorem not only establishes {\em what} the inverse of $\compose{S}{T}$ {\em is}, it also duplicates the conclusion of \acronymref{theorem}{CIVLT} and also establishes the invertibility of $\compose{S}{T}$.  But somehow, the proof of \acronymref{theorem}{CIVLT} is nicer way to get this property.\end{para}
%
\begin{para}Does \acronymref{theorem}{ICLT} remind you of the flavor of any theorem we have seen about matrices?  (Hint:  Think about getting dressed.)  Hmmmm.\end{para}
%
\sageadvice{CIVLT}{Computing the Inverse of a Linear Transformations}{linear transformation!computing an inverse}
%
\end{subsect}
%
\begin{subsect}{SI}{Structure and Isomorphism}
%
\begin{para}A vector space is defined (\acronymref{definition}{VS}) as a set of objects (``vectors'') endowed with a definition of vector addition ($+$) and a definition of scalar multiplication (written with juxtaposition).  Many of our definitions about vector spaces involve linear combinations (\acronymref{definition}{LC}), such as the span of a set (\acronymref{definition}{SS}) and linear independence (\acronymref{definition}{LI}).  Other definitions are built up from these ideas, such as bases (\acronymref{definition}{B}) and dimension (\acronymref{definition}{D}).  The defining properties of a linear transformation require that a function ``respect'' the operations of the two vector spaces that are the domain and the codomain (\acronymref{definition}{LT}).  Finally, an invertible linear transformation is one that can be ``undone'' --- it has a companion that reverses its effect.  In this subsection we are going to begin to roll all these ideas into one.\end{para}
%
\begin{para}A vector space has ``structure'' derived from definitions of the two operations and the requirement that these operations interact in ways that satisfy the ten properties of \acronymref{definition}{VS}.  When two different vector spaces have an invertible linear transformation defined between them, then we can translate questions about linear combinations (spans, linear independence, bases, dimension) from the first vector space to the second.  The answers obtained in the second vector space can then be translated back, via the inverse linear transformation, and interpreted in the setting of the first vector space.  We say that these invertible linear transformations ``preserve structure.''  And we say that the two vector spaces are ``structurally the same.''  The precise term is ``isomorphic,'' from Greek meaning ``of the same form.''  Let's begin to try to understand this important concept.\end{para}
%
\begin{definition}{IVS}{Isomorphic Vector Spaces}{vector spaces!isomorphic}
\begin{para}Two vector spaces $U$ and $V$ are \define{isomorphic} if there exists an invertible linear transformation $T$ with domain $U$ and codomain $V$, $\ltdefn{T}{U}{V}$.  In this case, we write $U\isomorphic V$, and the linear transformation $T$ is known as an \define{isomorphism} between $U$ and $V$.\end{para}
\end{definition}
%
\begin{para}A few comments on this definition.  First, be careful with your language (\acronymref{technique}{L}).  Two vector spaces are isomorphic, or not.  It is a yes/no situation and the term only applies to a pair of vector spaces.  Any invertible linear transformation can be called an isomorphism, it is a term that applies to functions.  Second, a given pair of vector spaces there might be several different isomorphisms between the two vector spaces.  But it only takes the existence of one to call the pair isomorphic.  Third,  $U$ isomorphic to $V$, or $V$ isomorphic to $U$?  Doesn't matter, since the inverse linear transformation will provide the needed isomorphism in the ``opposite'' direction.  Being ``isomorphic to'' is an equivalence relation on the set of all vector spaces (see \acronymref{theorem}{SER} for a reminder about equivalence relations).\end{para}
%
\begin{example}{IVSAV}{Isomorphic vector spaces, Archetype V}{isomorphic!vector spaces}
\begin{para}\acronymref{archetype}{V} is a linear transformation from $P_3$ to $M_{22}$,
%
\begin{equation*}
\archetypepart{V}{ltdefn}\end{equation*}
\end{para}
%
\begin{para}Since it is injective and surjective, \acronymref{theorem}{ILTIS} tells us that it is an invertible linear transformation.  By \acronymref{definition}{IVS} we say $P_3$ and $M_{22}$ are isomorphic.\end{para}
%
\begin{para}At a basic level, the term ``isomorphic'' is nothing more than a codeword for the presence of an invertible linear transformation.  However, it is also a description of a powerful idea, and this power only becomes apparent in the course of studying examples and related theorems.  In this example, we are led to believe that there is nothing ``structurally'' different about $P_3$ and $M_{22}$.  In a certain sense they are the same.  Not equal, but the same.  One is as good as the other.  One is just as interesting as the other.\end{para}
%
\begin{para}Here is an extremely basic application of this idea.  Suppose we want to compute the following linear combination of polynomials in $P_3$,
%
\begin{equation*}
5(2+3x-4x^2+5x^3)+(-3)(3-5x+3x^2+x^3)
\end{equation*}
\end{para}
%
\begin{para}Rather than doing it straight-away (which is very easy), we will apply the transformation $T$ to convert into a linear combination of matrices, and then compute in $M_{22}$ according to the definitions of the vector space operations there (\acronymref{example}{VSM}),
%
\begin{align*}
%
&\lt{T}{5(2+3x-4x^2+5x^3)+(-3)(3-5x+3x^2+x^3)}\\
&=5\lt{T}{2+3x-4x^2+5x^3}+(-3)\lt{T}{3-5x+3x^2+x^3}&&\text{\acronymref{theorem}{LTLC}}\\
&=5
\begin{bmatrix}
5 & 10\\5 & -2
\end{bmatrix}
+(-3)
\begin{bmatrix}
-2 & -3\\1 & -6
\end{bmatrix}&&\text{Definition of $T$}\\
&=
\begin{bmatrix}
31 & 59\\22 & 8
\end{bmatrix}&&\text{Operations in $M_{22}$}
%
\end{align*}
\end{para}
%
\begin{para}Now we will translate our answer back to $P_3$ by applying $\ltinverse{T}$, which we found in \acronymref{example}{AIVLT},
%
\begin{equation*}
\archetypepart{V}{ltinverse}\end{equation*}
\end{para}
%
\begin{para}We compute,
%
\begin{equation*}
\lt{\ltinverse{T}}{
\begin{bmatrix}
31 & 59\\22 & 8
\end{bmatrix}
}
=1+30x-29x^2+22x^3
\end{equation*}
%
which is, as expected, exactly what we would have computed for the original linear combination had we just used the definitions of the operations in $P_3$ (\acronymref{example}{VSP}).  Notice this is meant only as an {\em illustration} and not a suggested route for doing this particular computation.\end{para}
%
\end{example}
%
\begin{para}Checking the dimensions of two vector spaces can be a quick way to establish that they are not isomorphic.  Here's the theorem.\end{para}
%
\begin{theorem}{IVSED}{Isomorphic Vector Spaces have Equal Dimension}{isomorphic vector spaces!dimension}
\begin{para}Suppose $U$ and $V$ are isomorphic vector spaces.  Then $\dimension{U}=\dimension{V}$.\end{para}
\end{theorem}
%
\begin{proof}
\begin{para}If $U$ and $V$ are isomorphic, there is an invertible linear transformation $\ltdefn{T}{U}{V}$ (\acronymref{definition}{IVS}).  $T$ is injective by \acronymref{theorem}{ILTIS} and so by \acronymref{theorem}{ILTD}, $\dimension{U}\leq\dimension{V}$.  Similarly, $T$ is surjective by \acronymref{theorem}{ILTIS} and so by \acronymref{theorem}{SLTD}, $\dimension{U}\geq\dimension{V}$.  The net effect of these two inequalities is that $\dimension{U}=\dimension{V}$.\end{para}
\end{proof}
%
\begin{para}The contrapositive of \acronymref{theorem}{IVSED} says that if $U$ and $V$ have different dimensions, then they are not isomorphic.  Dimension is the simplest ``structural'' characteristic that will allow you to distinguish non-isomorphic vector spaces.  For example $P_6$ is not isomorphic to $M_{34}$ since their dimensions (7 and 12, respectively) are not equal.  With tools developed in \acronymref{section}{VR} we will be able to establish that the converse of \acronymref{theorem}{IVSED} is true.  Think about that one for a moment.\end{para}
%
\end{subsect}
%
\begin{subsect}{RNLT}{Rank and Nullity of a Linear Transformation}
%
\begin{para}Just as a matrix has a rank and a nullity, so too do linear transformations.  And just like the rank and nullity of a matrix are related (they sum to the number of columns, \acronymref{theorem}{RPNC}) the rank and nullity of a linear transformation are related.  Here are the definitions and theorems, see the Archetypes (\acronymref{appendix}{A}) for loads of examples.\end{para}
%
\begin{definition}{ROLT}{Rank Of a Linear Transformation}{rank!linear transformation}
\begin{para}Suppose that $\ltdefn{T}{U}{V}$ is a linear transformation.  Then the \define{rank} of $T$, $\rank{T}$, is the dimension of the range of $T$,
%
\begin{equation*}
\rank{T}=\dimension{\rng{T}}
\end{equation*}\end{para}
%
\denote{ROLT}{Rank of a Linear Transformation}{$\rank{T}$}{rank}
\end{definition}
%
\begin{definition}{NOLT}{Nullity Of a Linear Transformation}{nullity!linear transformation}
\begin{para}Suppose that $\ltdefn{T}{U}{V}$ is a linear transformation.  Then the \define{nullity} of $T$, $\nullity{T}$, is the dimension of the kernel of $T$,
%
\begin{equation*}
\nullity{T}=\dimension{\krn{T}}
\end{equation*}\end{para}
%
\denote{NOLT}{Nullity of a Linear Transformation}{$\nullity{T}$}{nullity}
\end{definition}
%
\begin{para}Here are two quick theorems.\end{para}
%
\begin{theorem}{ROSLT}{Rank Of a Surjective Linear Transformation}{rank!surjective linear transformation}
\begin{para}Suppose that $\ltdefn{T}{U}{V}$ is a linear transformation.  Then the rank of $T$ is the dimension of $V$, $\rank{T}=\dimension{V}$, if and only if $T$ is surjective.\end{para}
\end{theorem}
%
\begin{proof}
\begin{para}By \acronymref{theorem}{RSLT}, $T$ is surjective if and only if $\rng{T}=V$.  Applying \acronymref{definition}{ROLT}, $\rng{T}=V$ if and only if $\rank{T}=\dimension{\rng{T}}=\dimension{V}$.\end{para}
\end{proof}
%
\begin{theorem}{NOILT}{Nullity Of an Injective Linear Transformation}{nullity!injective linear transformation}
\begin{para}Suppose that $\ltdefn{T}{U}{V}$ is a linear transformation.  Then the nullity of $T$ is zero, $\nullity{T}=0$, if and only if $T$ is injective.\end{para}
\end{theorem}
%
\begin{proof}
\begin{para}By \acronymref{theorem}{KILT}, $T$ is injective if and only if $\krn{T}=\set{\zerovector}$.  Applying \acronymref{definition}{NOLT}, $\krn{T}=\set{\zerovector}$ if and only if $\nullity{T}=0$.\end{para}
\end{proof}
%
\begin{para}Just as injectivity and surjectivity come together in invertible linear transformations, there is a clear relationship between rank and nullity of a linear transformation.  If one is big, the other is small.\end{para}
%
\begin{theorem}{RPNDD}{Rank Plus Nullity is Domain Dimension}{linear transformation!rank plus nullity}
\begin{para}Suppose that $\ltdefn{T}{U}{V}$ is a linear transformation.  Then
%
\begin{equation*}
\rank{T}+\nullity{T}=\dimension{U}
\end{equation*}
\end{para}
%
\end{theorem}
%
\begin{proof}
\begin{para}Let $r=\rank{T}$ and $s=\nullity{T}$.  Suppose that $R=\set{\vectorlist{v}{r}}\subseteq V$ is a basis of the range of $T$, $\rng{T}$, and $S=\set{\vectorlist{u}{s}}\subseteq U$ is a basis of the kernel of $T$, $\krn{T}$.  Note that $R$ and $S$ are possibly empty, which means that some of the sums in this proof are ``empty'' and are equal to the zero vector.\end{para}
%
\begin{para}Because the elements of $R$ are all in the range of $T$, each must have a non-empty pre-image by \acronymref{theorem}{RPI}.  Choose vectors $\vect{w}_i\in U$, $1\leq i\leq r$ such that $\vect{w}_i\in\preimage{T}{\vect{v}_i}$.  So $\lt{T}{\vect{w}_i}=\vect{v}_i$, $1\leq i\leq r$.  Consider the set
%
\begin{equation*}
B=\set{\vectorlist{u}{s},\,\vectorlist{w}{r}}
\end{equation*}

%
We claim that $B$ is a basis for $U$.\end{para}
%
\begin{para}To establish linear independence for $B$, begin with a relation of linear dependence on $B$.  So suppose there are scalars $\scalarlist{a}{s}$ and $\scalarlist{b}{r}$
%
\begin{equation*}
\zerovector=\lincombo{a}{u}{s}+\lincombo{b}{w}{r}
\end{equation*}
\end{para}
%
\begin{para}Then
%
\begin{align*}
\zerovector&=\lt{T}{\zerovector}&&\text{\acronymref{theorem}{LTTZZ}}\\
%
&=T\left(\lincombo{a}{u}{s}+\right.\\
&\quad\quad\quad\quad\left.\lincombo{b}{w}{r}\right)&&\text{\acronymref{definition}{LI}}\\
%
&=a_1\lt{T}{\vect{u}_1}+a_2\lt{T}{\vect{u}_2}+a_3\lt{T}{\vect{u}_3}+\cdots+a_s\lt{T}{\vect{u}_s}+\\
&\quad\quad b_1\lt{T}{\vect{w}_1}+b_2\lt{T}{\vect{w}_2}+b_3\lt{T}{\vect{w}_3}+\cdots+b_r\lt{T}{\vect{w}_r}
&&\text{\acronymref{theorem}{LTLC}}\\
%
&=a_1\zerovector+a_2\zerovector+a_3\zerovector+\cdots+a_s\zerovector+\\
&\quad\quad b_1\lt{T}{\vect{w}_1}+b_2\lt{T}{\vect{w}_2}+b_3\lt{T}{\vect{w}_3}+\cdots+b_r\lt{T}{\vect{w}_r}
&&\text{\acronymref{definition}{KLT}}\\
%
&=\zerovector+\zerovector+\zerovector+\cdots+\zerovector+\\
&\quad\quad b_1\lt{T}{\vect{w}_1}+b_2\lt{T}{\vect{w}_2}+b_3\lt{T}{\vect{w}_3}+\cdots+b_r\lt{T}{\vect{w}_r}
&&\text{\acronymref{theorem}{ZVSM}}\\
%
&=b_1\lt{T}{\vect{w}_1}+b_2\lt{T}{\vect{w}_2}+b_3\lt{T}{\vect{w}_3}+\cdots+b_r\lt{T}{\vect{w}_r}&&\text{\acronymref{property}{Z}}\\
%
&=b_1\vect{v}_1+b_2\vect{v}_2+b_3\vect{v}_3+\cdots+b_r\vect{v}_r&&\text{\acronymref{definition}{PI}}
\end{align*}
\end{para}
%
\begin{para}This is a relation of linear dependence on $R$ (\acronymref{definition}{RLD}), and since $R$ is a linearly independent set (\acronymref{definition}{LI}), we see that $b_1=b_2=b_3=\ldots=b_r=0$.  Then the original relation of linear dependence on $B$ becomes
%
\begin{align*}
\zerovector&=\lincombo{a}{u}{s}+0\vect{w}_1+0\vect{w}_2+\ldots+0\vect{w}_r\\
&=\lincombo{a}{u}{s}+\zerovector+\zerovector+\ldots+\zerovector&&\text{\acronymref{theorem}{ZSSM}}\\
&=\lincombo{a}{u}{s}&&\text{\acronymref{property}{Z}}
\end{align*}
\end{para}
%
\begin{para}But this is again a relation of linear independence (\acronymref{definition}{RLD}), now on the set $S$.  Since $S$ is linearly independent (\acronymref{definition}{LI}), we have $a_1=a_2=a_3=\ldots=a_r=0$.  Since we now know that all the scalars in the relation of linear dependence on $B$ must be zero, we have established the linear independence of $S$ through \acronymref{definition}{LI}.\end{para}
%
\begin{para}To now establish that $B$ spans $U$, choose an arbitrary vector $\vect{u}\in U$.  Then $\lt{T}{\vect{u}}\in R(T)$, so there are scalars $\scalarlist{c}{r}$ such that
%
\begin{equation*}
\lt{T}{\vect{u}}=\lincombo{c}{v}{r}
\end{equation*}
\end{para}
%
\begin{para}Use the scalars $\scalarlist{c}{r}$ to define a vector $\vect{y}\in U$,
%
\begin{equation*}
\vect{y}=\lincombo{c}{w}{r}
\end{equation*}
\end{para}
%
\begin{para}Then
%
\begin{align*}
\lt{T}{\vect{u}-\vect{y}}&=\lt{T}{\vect{u}}-\lt{T}{\vect{y}}&&\text{\acronymref{theorem}{LTLC}}\\
%
&=\lt{T}{\vect{u}}-\lt{T}{\lincombo{c}{w}{r}}&&\text{Substitution}\\
%
&=\lt{T}{\vect{u}}-\left(c_1\lt{T}{\vect{w}_1}+c_2\lt{T}{\vect{w}_2}+\cdots+c_r\lt{T}{\vect{w}_r}\right)&&\text{\acronymref{theorem}{LTLC}}\\
%
&=\lt{T}{\vect{u}}-\left(\lincombo{c}{v}{r}\right)&&\vect{w}_i\in\preimage{T}{\vect{v}_i}\\
%
&=\lt{T}{\vect{u}}-\lt{T}{\vect{u}}&&\text{Substitution}\\
%
&=\zerovector&&\text{\acronymref{property}{AI}}
\end{align*}
\end{para}
%
\begin{para}So the vector $\vect{u}-\vect{y}$ is sent to the zero vector by $T$ and hence is an element of the kernel of $T$.  As such it can be written as a linear combination of the basis vectors for $\krn{T}$, the elements of the set $S$.  So there are scalars $\scalarlist{d}{s}$ such that
%
\begin{equation*}
\vect{u}-\vect{y}=\lincombo{d}{u}{s}
\end{equation*}
\end{para}
%
\begin{para}Then
%
\begin{align*}
\vect{u}&=\left(\vect{u}-\vect{y}\right)+\vect{y}\\
&=\lincombo{d}{u}{s}+\lincombo{c}{w}{r}
\end{align*}
\end{para}
%
\begin{para}This says that for any vector, $\vect{u}$, from $U$, there exist scalars ($\scalarlist{d}{s},\,\scalarlist{c}{r}$) that form $\vect{u}$ as a linear combination of the vectors in the set $B$.  In other words, $B$ spans $U$ (\acronymref{definition}{SS}).\end{para}
%
\begin{para}So $B$ is a basis (\acronymref{definition}{B}) of $U$ with $s+r$ vectors, and thus
%
\begin{equation*}
\dimension{U}=s+r=\nullity{T}+\rank{T}
\end{equation*}
%
as desired.\end{para}
%
\end{proof}
%
\begin{para}\acronymref{theorem}{RPNC} said that the rank and nullity of a matrix sum to the number of columns of the matrix.  This result is now an easy consequence of \acronymref{theorem}{RPNDD} when we consider the linear transformation $\ltdefn{T}{\complex{n}}{\complex{m}}$ defined with the $m\times n$ matrix $A$ by $\lt{T}{\vect{x}}=A\vect{x}$.  The range and kernel of $T$ are identical to the column space and null space of the matrix $A$ (\acronymref{exercise}{ILT.T20}, \acronymref{exercise}{SLT.T20}), so the rank and nullity of the matrix $A$ are identical to the rank and nullity of the linear transformation $T$.  The dimension of the domain of $T$ is the dimension of $\complex{n}$, exactly the number of columns for the matrix $A$.\end{para}
%
\begin{para}This theorem can be especially useful in determining basic properties of linear transformations.  For example, suppose that $\ltdefn{T}{\complex{6}}{\complex{6}}$ is a linear transformation and you are able to quickly establish that the kernel is trivial.  Then $\nullity{T}=0$.  First this means that $T$ is injective by \acronymref{theorem}{NOILT}.  Also, \acronymref{theorem}{RPNDD} becomes
%
\begin{equation*}
6=\dimension{\complex{6}}=\rank{T}+\nullity{T}=\rank{T}+0=\rank{T}
\end{equation*}
%
So the rank of $T$ is equal to the rank of the codomain, and by \acronymref{theorem}{ROSLT} we know $T$ is surjective.  Finally, we know $T$ is invertible by \acronymref{theorem}{ILTIS}.  So from the determination that the kernel is trivial, and consideration of various dimensions, the theorems of this section allow us to conclude the existence of an inverse linear transformation for $T$.
%
Similarly, \acronymref{theorem}{RPNDD} can be used to provide alternative proofs for \acronymref{theorem}{ILTD}, \acronymref{theorem}{SLTD} and \acronymref{theorem}{IVSED}.  It would be an interesting exercise to construct these proofs.\end{para}
%
\begin{para}It would be instructive to study the archetypes that are linear transformations and see how many of their properties can be deduced just from considering only the dimensions of the domain and codomain.  Then add in just knowledge of either the nullity or rank, and so how much more you can learn about the linear transformation.  The table preceding all of the archetypes (\acronymref{appendix}{A}) could be a good place to start this analysis.\end{para}
%
\sageadvice{LTOE}{Linear Transformation Odds and Ends}{linear transformation!rank, nullity}
%
\end{subsect}
%
\begin{subsect}{SLELT}{Systems of Linear Equations and Linear Transformations}
%
\begin{para}This subsection does not really belong in this section, or any other section, for that matter.  It is just the right time to have a discussion about the connections between the central topic of linear algebra, linear transformations, and our motivating topic from \acronymref{chapter}{SLE}, systems of linear equations.  We will discuss several theorems we have seen already, but we will also make some forward-looking statements that will be justified in \acronymref{chapter}{R}.\end{para}
%
\begin{para}\acronymref{archetype}{D} and \acronymref{archetype}{E} are ideal examples to illustrate connections with linear transformations.  Both have the same coefficient matrix,
%
\begin{equation*}
D=\archetypepart{D}{purematrix}\end{equation*}
\end{para}
%
\begin{para}To apply the {\em theory} of linear transformations to these two archetypes, employ matrix multiplication (\acronymref{definition}{MM}) and define the linear transformation,
%
\begin{equation*}
\ltdefn{T}{\complex{4}}{\complex{3}},\quad \lt{T}{\vect{x}}=D\vect{x}
=x_1\colvector{2\\-3\\1}+
x_2\colvector{1\\4\\1}+
x_3\colvector{7\\-5\\4}+
x_4\colvector{-7\\-6\\-5}
\end{equation*}
\end{para}
%
\begin{para}\acronymref{theorem}{MBLT} tells us that $T$ is indeed a linear transformation.  \acronymref{archetype}{D} asks for solutions to $\linearsystem{D}{\vect{b}}$, where $\vect{b}=\colvector{8\\-12\\-4}$.  In the language of linear transformations this is equivalent to asking for $\preimage{T}{\vect{b}}$.  In the language of vectors and matrices it asks for a linear combination of the four columns of $D$ that will equal $\vect{b}$.   One solution listed is $\vect{w}=\colvector{7\\8\\1\\3}$.  With a non-empty preimage, \acronymref{theorem}{KPI} tells us that the complete solution set of the linear system is the preimage of $\vect{b}$,
%
\begin{equation*}
\vect{w}+\krn{T}=\setparts{\vect{w}+\vect{z}}{\vect{z}\in\krn{T}}
\end{equation*}
\end{para}
%
\begin{para}The kernel of the linear transformation $T$ is exactly the null space of the matrix $D$ (see \acronymref{exercise}{ILT.T20}),  so this approach to the solution set should be reminiscent of \acronymref{theorem}{PSPHS}.  The kernel of the linear transformation is the preimage of the zero vector, exactly equal to the solution set of the homogeneous system $\homosystem{D}$.  Since $D$ has a null space of dimension two, every preimage (and in particular the preimage of $\vect{b}$) is as ``big'' as a subspace of dimension two (but is not a subspace).\end{para}
%
\begin{para}\acronymref{archetype}{E} is identical to \acronymref{archetype}{D} but with a different vector of constants, $\vect{d}=\colvector{2\\3\\2}$.  We can use the same linear transformation $T$ to discuss this system of equations since the coefficient matrix is identical.  Now the set of solutions to $\linearsystem{D}{\vect{d}}$  is the pre-image of $\vect{d}$, $\preimage{T}{\vect{d}}$.  However, the vector $\vect{d}$ is not in the range of the linear transformation (nor is it in the column space of the matrix, since these two sets are equal by \acronymref{exercise}{SLT.T20}).  So the empty pre-image is equivalent to the inconsistency of the linear system.\end{para}
%
\begin{para}These two archetypes each have three equations in four variables, so either the resulting linear systems are inconsistent, or they are consistent and application of \acronymref{theorem}{CMVEI} tells us that the system has infinitely many solutions.  Considering these same parameters for the linear transformation, the dimension of the domain, $\complex{4}$, is four, while the codomain, $\complex{3}$, has dimension three.  Then
%
\begin{align*}
\nullity{T}&=\dimension{\complex{4}}-\rank{T}&&\text{\acronymref{theorem}{RPNDD}}\\
&=4-\dimension{\rng{T}}&&\text{\acronymref{definition}{ROLT}}\\
&\geq 4-3&&\text{$\rng{T}$ subspace of $\complex{3}$}\\
&=1
\end{align*}
\end{para}
%
\begin{para}So the kernel of $T$ is nontrivial simply by considering the dimensions of the domain (number of variables) and the codomain (number of equations).  Pre-images of elements of the codomain that are not in the range of $T$ are empty (inconsistent systems).  For elements of the codomain that are in the range of $T$ (consistent systems), \acronymref{theorem}{KPI} tells us that the pre-images are built from the kernel, and with a non-trivial kernel, these pre-images are infinite (infinitely many solutions).\end{para}
%
\begin{para}When do systems of equations have unique solutions?  Consider the system of linear equations $\linearsystem{C}{\vect{f}}$ and the linear transformation $\lt{S}{\vect{x}}=C\vect{x}$.  If $S$ has a trivial kernel, then pre-images will either be empty or be finite sets with single elements.  Correspondingly, the coefficient matrix $C$ will have a trivial null space and solution sets will either be empty (inconsistent) or contain a single solution (unique solution).  Should the matrix be square and have a trivial null space then we recognize the matrix as being nonsingular.  A square matrix means that the corresponding linear transformation, $T$, has equal-sized domain and codomain.  With a nullity of zero, $T$ is injective, and also \acronymref{theorem}{RPNDD} tells us that rank of $T$ is equal to the dimension of the domain, which in turn is equal to the dimension of the codomain.  In other words, $T$ is surjective.  Injective and surjective, and \acronymref{theorem}{ILTIS} tells us that $T$ is invertible.  Just as we can use the inverse of the coefficient matrix to find the unique solution of any linear system with a nonsingular coefficient matrix (\acronymref{theorem}{SNCM}), we can use the inverse of the linear transformation to construct the unique element of any pre-image (proof of \acronymref{theorem}{ILTIS}).\end{para}
%
\begin{para}The executive summary of this discussion is that to every coefficient matrix of a system of linear equations we can associate a natural linear transformation.  Solution sets for systems with this coefficient matrix are preimages of elements of the codomain of the linear transformation.  For every theorem about systems of linear equations there is an analogue about linear transformations.  The theory of linear transformations provides all the tools to recreate the theory of solutions to linear systems of equations.\end{para}
%
\begin{para}We will continue this adventure in \acronymref{chapter}{R}.\end{para}
%
\sageadvice{SUTH1}{Sage Under The Hood, Round 1}{sage under the hood!round 1}
%
\end{subsect}
%
%  End of  ivlt.tex