%%%%(c)
%%%%(c)  This file is a portion of the source for the textbook
%%%%(c)
%%%%(c)    A First Course in Linear Algebra
%%%%(c)    Copyright 2004 by Robert A. Beezer
%%%%(c)
%%%%(c)  See the file COPYING.txt for copying conditions
%%%%(c)
%%%%(c)
%%%%%%%%%%%
%%
%%  Section D
%%  Dimension
%%
%%%%%%%%%%%
%
\begin{introduction}
\begin{para}Almost every vector space we have encountered has been infinite in size (an exception is \acronymref{example}{VSS}).  But some are bigger and richer than others.  Dimension, once suitably defined, will be a measure of the size of a vector space, and a useful tool for studying its properties.  You probably already have a rough notion of what a mathematical definition of dimension might be --- try to forget these imprecise ideas and go with the new ones given here.\end{para}
\end{introduction}
%
\begin{subsect}{D}{Dimension}
%
\begin{definition}{D}{Dimension}{dimension}
\begin{para}Suppose that $V$ is a vector space and $\set{\vectorlist{v}{t}}$ is a basis of $V$.  Then the \define{dimension} of $V$ is defined by $\dimension{V}=t$.  If $V$ has no finite bases, we say $V$ has infinite dimension.\end{para}
\denote{D}{Dimension}{$\dimension{V}$}{dimension}
\end{definition}
%
\begin{para}This is a very simple definition, which belies its power.  Grab a basis, any basis, and count up the number of vectors it contains.  That's the dimension.  However, this simplicity causes a problem.  Given a vector space, you and I could each construct different bases --- remember that a vector space might have many bases.  And what if your basis and my basis had different sizes?  Applying \acronymref{definition}{D} we would arrive at different numbers!  With our current knowledge about vector spaces, we would have to say that dimension is not ``well-defined.''  Fortunately, there is a theorem that will correct this problem.\end{para}
%
\begin{para}In a strictly logical progression, the next two theorems would {\em precede} the definition of dimension.  Many subsequent theorems will trace their lineage back to the following fundamental result.\end{para}
%
\begin{theorem}{SSLD}{Spanning Sets and Linear Dependence}{spanning set!more vectors}
\begin{para}Suppose that $S=\set{\vectorlist{v}{t}}$ is a finite set of vectors which spans the vector space $V$.  Then any set of $t+1$ or more vectors from $V$ is linearly dependent.\end{para}
\end{theorem}
%
\begin{proof}
\begin{para}We want to prove that any set of $t+1$ or more vectors from $V$ is linearly dependent.  So we will begin with a totally arbitrary set of vectors from $V$, $R=\set{\vectorlist{u}{m}}$, where $m>t$.  We will now construct a nontrivial relation of linear dependence on $R$.\end{para}
%
\begin{para}Each vector $\vectorlist{u}{m}$ can be written as a linear combination of $\vectorlist{v}{t}$ since $S$ is a spanning set of $V$.  This means there exist scalars  $a_{ij}$, $1\leq i\leq t$, $1\leq j\leq m$, so that
%
\begin{align*}
\vect{u}_1&=a_{11}\vect{v}_1+a_{21}\vect{v}_2+a_{31}\vect{v}_3+\cdots+a_{t1}\vect{v}_t\\
\vect{u}_2&=a_{12}\vect{v}_1+a_{22}\vect{v}_2+a_{32}\vect{v}_3+\cdots+a_{t2}\vect{v}_t\\
\vect{u}_3&=a_{13}\vect{v}_1+a_{23}\vect{v}_2+a_{33}\vect{v}_3+\cdots+a_{t3}\vect{v}_t\\
&\quad\quad\vdots\\
\vect{u}_m&=a_{1m}\vect{v}_1+a_{2m}\vect{v}_2+a_{3m}\vect{v}_3+\cdots+a_{tm}\vect{v}_t
\end{align*}
\end{para}
%
\begin{para}Now we form, unmotivated, the homogeneous system of $t$ equations in the $m$ variables, $x_1,\,x_2,\,x_3,\,\ldots,\,x_m$, where the coefficients are the just-discovered scalars $a_{ij}$,
%
\begin{align*}
a_{11}x_1+a_{12}x_2+a_{13}x_3+\cdots+a_{1m}x_m&=0\\
a_{21}x_1+a_{22}x_2+a_{23}x_3+\cdots+a_{2m}x_m&=0\\
a_{31}x_1+a_{32}x_2+a_{33}x_3+\cdots+a_{3m}x_m&=0\\
\vdots\quad\quad&\\
a_{t1}x_1+a_{t2}x_2+a_{t3}x_3+\cdots+a_{tm}x_m&=0\\
\end{align*}
\end{para}
%
\begin{para}This is a homogeneous system with more variables than equations (our hypothesis is expressed as $m>t$), so by \acronymref{theorem}{HMVEI} there are infinitely many solutions.  Choose a nontrivial solution and denote it by $x_1=c_1,\,x_2=c_2,\,x_3=c_3,\,\ldots,\,x_m=c_m$.  As a solution to the homogeneous system, we then have
%
\begin{align*}
a_{11}c_1+a_{12}c_2+a_{13}c_3+\cdots+a_{1m}c_m&=0\\
a_{21}c_1+a_{22}c_2+a_{23}c_3+\cdots+a_{2m}c_m&=0\\
a_{31}c_1+a_{32}c_2+a_{33}c_3+\cdots+a_{3m}c_m&=0\\
\vdots\quad\quad&\\
a_{t1}c_1+a_{t2}c_2+a_{t3}c_3+\cdots+a_{tm}c_m&=0\\
\end{align*}
\end{para}
%
\begin{para}As a collection of nontrivial scalars, $c_1,\,c_2,\,c_3,\,\dots,\,c_m$ will provide the nontrivial relation of linear dependence we desire,
%
\begin{align*}
&\lincombo{c}{u}{m}\\
%
&=c_{1}\left(a_{11}\vect{v}_1+a_{21}\vect{v}_2+a_{31}\vect{v}_3+\cdots+a_{t1}\vect{v}_t\right)
&&\text{\acronymref{definition}{TSVS}}\\
&\quad\quad+c_{2}\left(a_{12}\vect{v}_1+a_{22}\vect{v}_2+a_{32}\vect{v}_3+\cdots+a_{t2}\vect{v}_t\right)\\
&\quad\quad+c_{3}\left(a_{13}\vect{v}_1+a_{23}\vect{v}_2+a_{33}\vect{v}_3+\cdots+a_{t3}\vect{v}_t\right)\\
&\quad\quad\quad\quad\vdots\\
&\quad\quad+c_{m}\left(a_{1m}\vect{v}_1+a_{2m}\vect{v}_2+a_{3m}\vect{v}_3+\cdots+a_{tm}\vect{v}_t\right)\\
%
&=c_{1}a_{11}\vect{v}_1+c_{1}a_{21}\vect{v}_2+c_{1}a_{31}\vect{v}_3+\cdots+c_{1}a_{t1}\vect{v}_t
&&\text{\acronymref{property}{DVA}}\\
&\quad\quad+c_{2}a_{12}\vect{v}_1+c_{2}a_{22}\vect{v}_2+c_{2}a_{32}\vect{v}_3+\cdots+c_{2}a_{t2}\vect{v}_t\\
&\quad\quad+c_{3}a_{13}\vect{v}_1+c_{3}a_{23}\vect{v}_2+c_{3}a_{33}\vect{v}_3+\cdots+c_{3}a_{t3}\vect{v}_t\\
&\quad\quad\quad\quad\vdots\\
&\quad\quad+c_{m}a_{1m}\vect{v}_1+c_{m}a_{2m}\vect{v}_2+c_{m}a_{3m}\vect{v}_3+\cdots+c_{m}a_{tm}\vect{v}_t\\
%
&=\left(c_{1}a_{11}+c_{2}a_{12}+c_{3}a_{13}+\cdots+c_{m}a_{1m}\right)\vect{v}_1
&&\text{\acronymref{property}{DSA}}\\
&\quad\quad+\left(c_{1}a_{21}+c_{2}a_{22}+c_{3}a_{23}+\cdots+c_{m}a_{2m}\right)\vect{v}_2\\
&\quad\quad+\left(c_{1}a_{31}+c_{2}a_{32}+c_{3}a_{33}+\cdots+c_{m}a_{3m}\right)\vect{v}_3\\
&\quad\quad\quad\quad\vdots\\
&\quad\quad+\left(c_{1}a_{t1}+c_{2}a_{t2}+c_{3}a_{t3}+\cdots+c_{m}a_{tm}\right)\vect{v}_t\\
%
&=\left(a_{11}c_{1}+a_{12}c_{2}+a_{13}c_{3}+\cdots+a_{1m}c_{m}\right)\vect{v}_1
&&\text{\acronymref{property}{CMCN}}\\
%
&\quad\quad+\left(a_{21}c_{1}+a_{22}c_{2}+a_{23}c_{3}+\cdots+a_{2m}c_{m}\right)\vect{v}_2\\
&\quad\quad+\left(a_{31}c_{1}+a_{32}c_{2}+a_{33}c_{3}+\cdots+a_{3m}c_{m}\right)\vect{v}_3\\
&\quad\quad\quad\quad\vdots\\
&\quad\quad+\left(a_{t1}c_{1}+a_{t2}c_{2}+a_{t3}c_{3}+\cdots+a_{tm}c_{m}\right)\vect{v}_t\\
%
&=0\vect{v}_1+0\vect{v}_2+0\vect{v}_3+\cdots+0\vect{v}_t
&&\text{$c_j$ as solution}\\
%
&=\zerovector+\zerovector+\zerovector+\cdots+\zerovector
&&\text{\acronymref{theorem}{ZSSM}}\\
%
&=\zerovector
&&\text{\acronymref{property}{Z}}
%
\end{align*}
\end{para}
%
\begin{para}That does it.  $R$ has been undeniably shown to be a linearly dependent set.\end{para}
%
\begin{para}The proof just given has some monstrous expressions in it, mostly owing to the double subscripts present.  Now is a great opportunity to show the value of a more compact notation.  We will rewrite the key steps of the previous proof using summation notation, resulting in a more economical presentation, and even greater insight into the key aspects of the proof.  So here is an alternate proof --- study it carefully.\end{para}
%
\begin{para}{\bf Alternate Proof:}  We want to prove that any set of $t+1$ or more vectors from $V$ is linearly dependent.  So we will begin with a totally arbitrary set of vectors from $V$, $R=\setparts{\vect{u}_j}{1\leq j\leq m}$, where $m>t$.  We will now construct a nontrivial relation of linear dependence on $R$.\end{para}
%
\begin{para}Each vector $\vect{u_j}$, $1\leq j\leq m$ can be written as a linear combination of $\vect{v}_i$, $1\leq i\leq t$ since $S$ is a spanning set of $V$.  This means there are scalars  $a_{ij}$, $1\leq i\leq t$, $1\leq j\leq m$, so that
%
\begin{align*}
\vect{u}_j&=\sum_{i=1}^{t}a_{ij}\vect{v_i}&&1\leq j\leq m
\end{align*}\end{para}
%
\begin{para}Now we form, unmotivated, the homogeneous system of $t$ equations in the $m$ variables, $x_j$, $1\leq j\leq m$, where the coefficients are the just-discovered scalars $a_{ij}$,
%
\begin{align*}
\sum_{j=1}^{m}a_{ij}x_j=0&&1\leq i\leq t
\end{align*}
\end{para}
%
\begin{para}This is a homogeneous system with more variables than equations (our hypothesis is expressed as $m>t$), so by \acronymref{theorem}{HMVEI} there are infinitely many solutions.  Choose one of these solutions that is not trivial and denote it by $x_j=c_j$, $1\leq j\leq m$.  As a solution to the homogeneous system, we then have $\sum_{j=1}^{m}a_{ij}c_{j}=0$ for $1\leq i\leq t$.  As a collection of nontrivial scalars, $c_j$, $1\leq j\leq m$, will provide the nontrivial relation of linear dependence we desire,
%
\begin{align*}
\sum_{j=1}^{m}c_{j}\vect{u}_j
&=\sum_{j=1}^{m}c_{j}\left(\sum_{i=1}^{t}a_{ij}\vect{v_i}\right)
&&\text{\acronymref{definition}{TSVS}}\\
%
&=\sum_{j=1}^{m}\sum_{i=1}^{t}c_{j}a_{ij}\vect{v}_i
&&\text{\acronymref{property}{DVA}}\\
%
&=\sum_{i=1}^{t}\sum_{j=1}^{m}c_{j}a_{ij}\vect{v}_i
&&\text{\acronymref{property}{CMCN}}\\
%
&=\sum_{i=1}^{t}\sum_{j=1}^{m}a_{ij}c_{j}\vect{v}_i
&&\text{Commutativity in $\complex{}$}\\
%
&=\sum_{i=1}^{t}\left(\sum_{j=1}^{m}a_{ij}c_{j}\right)\vect{v}_i
&&\text{\acronymref{property}{DSA}}\\
%
&=\sum_{i=1}^{t}0\vect{v}_i
&&\text{$c_j$ as solution}\\
%
&=\sum_{i=1}^{t}\zerovector
&&\text{\acronymref{theorem}{ZSSM}}\\
%
&=\zerovector
&&\text{\acronymref{property}{Z}}
%
\end{align*}
\end{para}
%
\begin{para}That does it.  $R$ has been undeniably shown to be a linearly dependent set.\end{para}
\end{proof}
%
\begin{para}Notice how the swap of the two summations is so much easier in the third step above, as opposed to all the rearranging and regrouping that takes place in the previous proof.  In about half the space.  And there are no ellipses ($\ldots$).\end{para}
%
\begin{para}\acronymref{theorem}{SSLD} can be viewed as a generalization of \acronymref{theorem}{MVSLD}.  We know that $\complex{m}$ has a basis with $m$ vectors in it (\acronymref{theorem}{SUVB}), so it is a set of $m$ vectors that spans $\complex{m}$.  By \acronymref{theorem}{SSLD}, any set of more than $m$ vectors from $\complex{m}$ will be linearly dependent.  But this is exactly the conclusion we have in \acronymref{theorem}{MVSLD}.  Maybe this is not a total shock, as the proofs of both theorems rely heavily on \acronymref{theorem}{HMVEI}.   The beauty of \acronymref{theorem}{SSLD} is that it applies in any vector space.  We illustrate the generality of this theorem, and hint at its power, in the next example.\end{para}
%
\begin{example}{LDP4}{Linearly dependent set in $P_4$}{linearly dependent set!polynomials}
\begin{para}In \acronymref{example}{SSP4} we showed that
%
\begin{equation*}
S=\set{x-2,\,x^2-4x+4,\,x^3-6x^2+12x-8,\,x^4-8x^3+24x^2-32x+16}
\end{equation*}
%
is a spanning set for $W=\setparts{p(x)}{p\in P_4,\ p(2)=0}$.  So we can apply \acronymref{theorem}{SSLD} to $W$ with $t=4$.  Here is a set of five vectors from $W$, as you may check by verifying that each is a polynomial of degree 4 or less and has $x=2$ as a root,
%
\begin{align*}
T&=\set{p_1,\,p_2,\,p_3,\,p_4,\,p_5}\subseteq W\\
&\ \\
p_1&=x^4-2x^3+2x^2-8x+8\\
p_2&=-x^3+6x^2-5x-6\\
p_3&=2x^4-5x^3+5x^2-7x+2\\
p_4&=-x^4+4x^3-7x^2+6x\\
p_5&=4x^3-9x^2+5x-6
\end{align*}
\end{para}
%
\begin{para}By \acronymref{theorem}{SSLD} we conclude that $T$ is linearly dependent, with no further computations.
\end{para}
\end{example}
%
\begin{para}\acronymref{theorem}{SSLD} is indeed powerful, but our main purpose in proving it right now was to make sure that our definition of dimension (\acronymref{definition}{D}) is well-defined.  Here's the theorem.\end{para}
%
%
\begin{theorem}{BIS}{Bases have Identical Sizes}{basis!common size}
\begin{para}Suppose that $V$ is a vector space with a finite basis $B$ and a second basis $C$.  Then $B$ and $C$ have the same size.\end{para}
\end{theorem}
%
\begin{proof}
\begin{para}Suppose that $C$ has more vectors than $B$.  (Allowing for the possibility that $C$ is infinite, we can replace $C$ by a subset that has more vectors than $B$.)  As a basis, $B$ is a spanning set for $V$ (\acronymref{definition}{B}), so \acronymref{theorem}{SSLD} says that $C$ is linearly dependent.  However, this contradicts the fact that as a basis $C$ is linearly independent (\acronymref{definition}{B}).  So $C$ must also be a finite set, with size less than, or equal to, that of $B$.\end{para}
%
\begin{para}Suppose that $B$ has more vectors than $C$.   As a basis, $C$ is a spanning set for $V$ (\acronymref{definition}{B}), so \acronymref{theorem}{SSLD} says that $B$ is linearly dependent.  However, this contradicts the fact that as a basis $B$ is linearly independent (\acronymref{definition}{B}).  So $C$ cannot be strictly smaller than $B$.\end{para}
%
\begin{para}The only possibility left for the sizes of $B$ and $C$ is for them to be equal.\end{para}
%
\end{proof}
%
\begin{para}\acronymref{theorem}{BIS} tells us that if we find one finite basis in a vector space, then they all have the same size.  This (finally) makes \acronymref{definition}{D} unambiguous.\end{para}
%
\end{subsect}
%
\begin{subsect}{DVS}{Dimension of Vector Spaces}
%
\begin{para}We can now collect the dimension of some common, and not so common, vector spaces.\end{para}
%
%
\begin{theorem}{DCM}{Dimension of $\complex{m}$}{complex vector space!dimension}
\begin{para}The dimension of $\complex{m}$ (\acronymref{example}{VSCV}) is $m$.\end{para}
\end{theorem}
%
\begin{proof}
\begin{para}\acronymref{theorem}{SUVB} provides a basis with $m$ vectors.\end{para}
\end{proof}
%
\begin{theorem}{DP}{Dimension of $P_n$}{polynomial vector space!dimension}
\begin{para}The dimension of $P_{n}$  (\acronymref{example}{VSP}) is $n+1$.\end{para}
\end{theorem}
%
\begin{proof}
\begin{para}\acronymref{example}{BP} provides {\em two} bases with $n+1$ vectors.  Take your pick.\end{para}
\end{proof}
%
\begin{theorem}{DM}{Dimension of $M_{mn}$}{matrix vector space!dimension}
\begin{para}The dimension of $M_{mn}$  (\acronymref{example}{VSM}) is $mn$.\end{para}
\end{theorem}
%
\begin{proof}
\begin{para}\acronymref{example}{BM} provides a basis with $mn$ vectors.\end{para}
\end{proof}
%
\begin{example}{DSM22}{Dimension of a subspace of $M_{22}$}{dimension!subspace}
\begin{para}It should now be plausible that
%
\begin{equation*}
Z=\setparts{\begin{bmatrix}a&b\\c&d\end{bmatrix}}{2a+b+3c+4d=0,\,-a+3b-5c-2d=0}
\end{equation*}
%
is a subspace of the vector space $M_{22}$ (\acronymref{example}{VSM}).  (It is.)  To find the dimension of $Z$ we must first find a basis, though any old basis will do.\end{para}
%
\begin{para}First concentrate on the conditions relating $a,\,b,\,c$ and $d$.  They form a homogeneous system of two equations in four variables with coefficient matrix
%
\begin{equation*}
\begin{bmatrix}
2 & 1 & 3 & 4\\
-1 & 3 & -5 & -2
\end{bmatrix}
\end{equation*}
\end{para}
%
\begin{para}We can row-reduce this matrix to obtain
%
\begin{equation*}
\begin{bmatrix}
\leading{1} & 0 & 2 & 2\\
0 & \leading{1} & -1 & 0
\end{bmatrix}
\end{equation*}
\end{para}
%
\begin{para}Rewrite the two equations represented by each row of this matrix, expressing the dependent variables ($a$ and $b$) in terms of the free variables ($c$ and $d$), and we obtain,
%
\begin{align*}
a&=-2c-2d\\
b&=c
\end{align*}
\end{para}
%
\begin{para}We can now write a typical entry of $Z$ strictly in terms of $c$ and $d$, and we can decompose the result,
%
\begin{equation*}
\begin{bmatrix}a&b\\c&d\end{bmatrix}=
\begin{bmatrix}-2c-2d&c\\c&d\end{bmatrix}=
%
\begin{bmatrix}-2c&c\\c&0\end{bmatrix}+
\begin{bmatrix}-2d&0\\0&d\end{bmatrix}=
%
c\begin{bmatrix}-2&1\\1&0\end{bmatrix}+
d\begin{bmatrix}-2&0\\0&1\end{bmatrix}
%
\end{equation*}
\end{para}
%
\begin{para}This equation says that an arbitrary matrix in $Z$ can be written as a linear combination of the two vectors in
%
\begin{equation*}
S=\set{\begin{bmatrix}-2&1\\1&0\end{bmatrix},\,\begin{bmatrix}-2&0\\0&1\end{bmatrix}}
\end{equation*}
%
so we know that
%
\begin{equation*}
Z=\spn{S}=
\spn{\set{
\begin{bmatrix}-2&1\\1&0\end{bmatrix},\,
\begin{bmatrix}-2&0\\0&1\end{bmatrix}
}}
\end{equation*}
\end{para}
%
\begin{para}Are these two matrices (vectors) also linearly independent?  Begin with a relation of linear dependence on $S$,
%
\begin{align*}
a_1\begin{bmatrix}-2&1\\1&0\end{bmatrix}+
a_2\begin{bmatrix}-2&0\\0&1\end{bmatrix}&=\zeromatrix\\
\begin{bmatrix}-2a_1-2a_2&a_1\\a_1&a_2\end{bmatrix}&=
\begin{bmatrix}0&0\\0&0\end{bmatrix}
\end{align*}
\end{para}
%
\begin{para}From the equality of the two entries in the last row, we conclude that $a_1=0$, $a_2=0$.  Thus the only possible relation of linear dependence is the trivial one, and therefore $S$ is linearly independent (\acronymref{definition}{LI}).  So $S$ is a basis for $V$ (\acronymref{definition}{B}).  Finally, we can conclude that $\dimension{Z}=2$ (\acronymref{definition}{D}) since $S$ has two elements.\end{para}
%
\end{example}
%
\begin{example}{DSP4}{Dimension of a subspace of $P_4$}{dimension!polynomial subspace}
\begin{para}In \acronymref{example}{BSP4} we showed that
%
\begin{equation*}
S=\set{x-2,\,x^2-4x+4,\,x^3-6x^2+12x-8,\,x^4-8x^3+24x^2-32x+16}
\end{equation*}
%
is a basis for $W=\setparts{p(x)}{p\in P_4,\ p(2)=0}$.  Thus, the dimension of $W$ is four, $\dimension{W}=4$.\end{para}
%
\begin{para}Note that $\dimension{P_4}=5$ by \acronymref{theorem}{DP}, so $W$ is a subspace of dimension 4 within the vector space $P_4$ of dimension 5, illustrating the upcoming \acronymref{theorem}{PSSD}.\end{para}
%
\end{example}
%
%
\begin{example}{DC}{Dimension of the crazy vector space}{dimension!crazy vector space}
\begin{para}In \acronymref{example}{BC}  we determined that the set $R=\set{(1,\,0),\,(6,\,3)}$ from the crazy vector space, $C$ (\acronymref{example}{CVS}), is a basis for $C$.  By \acronymref{definition}{D} we see that $C$ has dimension 2, $\dimension{C}=2$.\end{para}
\end{example}
%
\begin{para}It is possible for a vector space to have no finite bases, in which case we say it has infinite dimension.  Many of the best examples of this are vector spaces of functions, which lead to constructions like Hilbert spaces.  We will focus exclusively on finite-dimensional vector spaces.  OK, one infinite-dimensional example, and {\em then} we will focus exclusively on finite-dimensional vector spaces.\end{para}
%
\begin{example}{VSPUD}{Vector space of polynomials with unbounded degree}{vector space!infinite dimension}
\begin{para}Define the set $P$ by
%
\begin{equation*}
P=\setparts{p}{p(x)\text{ is a polynomial in }x}
\end{equation*}\end{para}
%
\begin{para}Our operations will be the same as those defined for $P_n$ (\acronymref{example}{VSP}).\end{para}
%
\begin{para}With no restrictions on the possible degrees of our polynomials, any finite set that is a candidate for spanning $P$ will come up short.  We will give a proof by contradiction (\acronymref{technique}{CD}).  To this end, suppose that the dimension of $P$ is finite, say $\dimension{P}=n$.\end{para}
%
\begin{para}The set $T=\set{1,\,x,\,x^2,\,\ldots,\,x^n}$ is a linearly independent set (check this!) containing $n+1$ polynomials from $P$.  However, a basis of $P$ will be a spanning set of $P$ containing $n$ vectors.  This situation is a contradiction of \acronymref{theorem}{SSLD}, so our assumption that $P$ has finite dimension is false.  Thus, we say $\dimension{P}=\infty$.\end{para}
\end{example}
%
\sageadvice{D}{Dimension}{dimension}
%
\end{subsect}
%
\begin{subsect}{RNM}{Rank and Nullity of a Matrix}
%
\begin{para}For any matrix, we have seen that we can associate several subspaces --- the null space (\acronymref{theorem}{NSMS}), the column space (\acronymref{theorem}{CSMS}), row space (\acronymref{theorem}{RSMS}) and the left null space  (\acronymref{theorem}{LNSMS}).  As vector spaces, each of these has a dimension, and for the null space and column space, they are important enough to warrant names.\end{para}
%
\begin{definition}{NOM}{Nullity Of a Matrix}{nullity!matrix}
\begin{para}Suppose that $A$ is an $m\times n$ matrix.  Then the \define{nullity} of $A$ is the dimension of the null space of $A$, $\nullity{A}=\dimension{\nsp{A}}$.\end{para}
\denote{NOM}{Nullity of a Matrix}{$\nullity{A}$}{nullity}
\end{definition}
%
\begin{definition}{ROM}{Rank Of a Matrix}{rank!matrix}
\begin{para}Suppose that $A$ is an $m\times n$ matrix.  Then the \define{rank} of $A$ is the dimension of the column space of $A$, $\rank{A}=\dimension{\csp{A}}$.\end{para}
\denote{ROM}{Rank of a Matrix}{$\rank{A}$}{rank}
\end{definition}
%
\begin{example}{RNM}{Rank and nullity of a matrix}{rank!matrix}
\index{nullity!matrix}
\begin{para}Let's compute the rank and nullity of
%
\begin{equation*}
A=\begin{bmatrix}
2 & -4 & -1 & 3 & 2 & 1 & -4\\
1 & -2 & 0 & 0 & 4 & 0 & 1\\
-2 & 4 & 1 & 0 & -5 & -4 & -8\\
1 & -2 & 1 & 1 & 6 & 1 & -3\\
2 & -4 & -1 & 1 & 4 & -2 & -1\\
-1 & 2 & 3 & -1 & 6 & 3 & -1
\end{bmatrix}
\end{equation*}
\end{para}
%
\begin{para}To do this, we will first row-reduce the matrix since that will help us determine bases for the null space and column space.
%
\begin{equation*}
\begin{bmatrix}
\leading{1} & -2 & 0 & 0 & 4 & 0 & 1\\
0 & 0 & \leading{1} & 0 & 3 & 0 & -2\\
0 & 0 & 0 & \leading{1} & -1 & 0 & -3\\
0 & 0 & 0 & 0 & 0 & \leading{1} & 1\\
0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0
\end{bmatrix}
\end{equation*}
\end{para}
%
\begin{para}From this row-equivalent matrix in reduced row-echelon form we record $D=\set{1,\,3,\,4,\,6}$ and $F=\set{2,\,5,\,7}$.\end{para}
%
\begin{para}For each index in $D$, \acronymref{theorem}{BCS} creates a single basis vector.  In total the basis will have $4$ vectors, so the column space of $A$ will have dimension $4$ and we write $\rank{A}=4$.\end{para}
%
\begin{para}For each index in $F$, \acronymref{theorem}{BNS} creates a single basis vector.  In total the basis will have $3$ vectors, so the null space of $A$ will have dimension $3$ and we write $\nullity{A}=3$.\end{para}
%
\end{example}
%
\begin{para}There were no accidents or coincidences in the previous example --- with the row-reduced version of a matrix in hand, the rank and nullity are easy to compute.\end{para}
%
\begin{theorem}{CRN}{Computing Rank and Nullity}{rank!computing}
\index{nullity!computing}
\begin{para}Suppose that $A$ is an $m\times n$ matrix and $B$ is a row-equivalent matrix in reduced row-echelon form with $r$ nonzero rows.  Then $\rank{A}=r$ and $\nullity{A}=n-r$.\end{para}
\end{theorem}
%
\begin{proof}
\begin{para}\acronymref{theorem}{BCS} provides a basis for the column space by choosing columns of $A$ that correspond to the dependent variables in a description of the solutions to $\homosystem{A}$.  In the analysis of $B$, there is one dependent variable for each leading 1, one per nonzero row, or one per pivot column.  So there are $r$ column vectors in a basis for $\csp{A}$.\end{para}
%
\begin{para}\acronymref{theorem}{BNS} provide a basis for the null space by creating basis vectors of the null space of $A$ from entries of $B$, one for each independent variable, one per column with out a leading 1.  So there are $n-r$ column vectors in a basis for $\nullity{A}$.\end{para}
%
\end{proof}
%
\begin{para}Every archetype (\acronymref{appendix}{A}) that involves a matrix lists its rank and nullity.  You may have noticed as you studied the archetypes that the larger the column space is the smaller the null space is.  A simple corollary states this trade-off succinctly.    (See \acronymref{technique}{LC}.)\end{para}
%
\begin{theorem}{RPNC}{Rank Plus Nullity is Columns}{rank+nullity}
\begin{para}Suppose that $A$ is an $m\times n$ matrix.  Then $\rank{A}+\nullity{A}=n$.\end{para}
\end{theorem}
%
\begin{proof}
\begin{para}Let $r$ be the number of nonzero rows in a row-equivalent matrix in reduced row-echelon form.
By \acronymref{theorem}{CRN},
%
\begin{equation*}
\rank{A}+\nullity{A}= r+(n-r)=n
\end{equation*}\end{para}
%
\end{proof}
%
\begin{para}When we first introduced $r$ as our standard notation for the number of nonzero rows in a matrix in reduced row-echelon form you might have thought $r$ stood for ``rows.''  Not really --- it stands for ``rank''!\end{para}
%
\sageadvice{RNM}{Rank and Nullity of a Matrix}{rank, nullity of a matrix}
%
\end{subsect}
%
\begin{subsect}{RNNM}{Rank and Nullity of a Nonsingular Matrix}
%
\begin{para}Let's take a look at the rank and nullity of a square matrix.\end{para}
%
\begin{example}{RNSM}{Rank and nullity of a square matrix}{rank!square matrix}
\index{nullity!square matrix}
\begin{para}The matrix
%
\begin{equation*}
E=\begin{bmatrix}
0 & 4 & -1 & 2 & 2 & 3 & 1\\
2 & -2 & 1 & -1 & 0 & -4 & -3\\
-2 & -3 & 9 & -3 & 9 & -1 & 9\\
-3 & -4 & 9 & 4 & -1 & 6 & -2\\
-3 & -4 & 6 & -2 & 5 & 9 & -4\\
9 & -3 & 8 & -2 & -4 & 2 & 4\\
8 & 2 & 2 & 9 & 3 & 0 & 9
\end{bmatrix}
\end{equation*}
%
is row-equivalent to the matrix in reduced row-echelon form,
%
\begin{equation*}
\begin{bmatrix}
\leading{1} & 0 & 0 & 0 & 0 & 0 & 0\\
0 & \leading{1} & 0 & 0 & 0 & 0 & 0\\
0 & 0 & \leading{1} & 0 & 0 & 0 & 0\\
0 & 0 & 0 & \leading{1} & 0 & 0 & 0\\
0 & 0 & 0 & 0 & \leading{1} & 0 & 0\\
0 & 0 & 0 & 0 & 0 & \leading{1} & 0\\
0 & 0 & 0 & 0 & 0 & 0 & \leading{1}
\end{bmatrix}
\end{equation*}
\end{para}
%
\begin{para}With $n=7$ columns and $r=7$ nonzero rows \acronymref{theorem}{CRN} tells us the rank is $\rank{E}=7$ and the nullity is $\nullity{E}=7-7=0$.\end{para}
\end{example}
%
\begin{para}The value of either the nullity or the rank are enough to characterize a nonsingular matrix.\end{para}
%
\begin{theorem}{RNNM}{Rank and Nullity of a Nonsingular Matrix}{nonsingular matrix!rank}
\index{nonsingular matrix!nullity}
\begin{para}Suppose that $A$ is a square matrix of size $n$.  The following are equivalent.
%
\begin{enumerate}
\item A is nonsingular.
\item The rank of $A$ is $n$, $\rank{A}=n$.
\item The nullity of $A$ is zero, $\nullity{A}=0$.
\end{enumerate}
\end{para}
%
\end{theorem}
%
\begin{proof}
\begin{para}(1 $\Rightarrow$ 2)  \acronymref{theorem}{CSNM} says that if $A$ is nonsingular then $\csp{A}=\complex{n}$.  If $\csp{A}=\complex{n}$, then the column space has dimension $n$ by \acronymref{theorem}{DCM}, so the rank of $A$ is $n$.\end{para}
%
\begin{para}(2 $\Rightarrow$ 3)  Suppose $\rank{A}=n$.  Then \acronymref{theorem}{RPNC} gives
%
\begin{align*}
\nullity{A}&=n-\rank{A}&&\text{\acronymref{theorem}{RPNC}}\\
&=n-n&&\text{Hypothesis}\\
&=0
\end{align*}
\end{para}
%
\begin{para}(3 $\Rightarrow$ 1)  Suppose $\nullity{A}=0$, so a basis for the null space of $A$ is the empty set.  This implies that $\nsp{A}=\set{\zerovector}$ and \acronymref{theorem}{NMTNS} says $A$ is nonsingular.\end{para}
\end{proof}
%
\begin{para}With a new equivalence for a nonsingular matrix, we can update our list of equivalences (\acronymref{theorem}{NME5}) which now becomes a list requiring double digits to number.\end{para}
%
\begin{theorem}{NME6}{Nonsingular Matrix Equivalences, Round 6}{nonsingular matrix!equivalences}
\begin{para}Suppose that $A$ is a square matrix of size $n$.  The following are equivalent.
%
\begin{enumerate}
\item $A$ is nonsingular.
\item $A$ row-reduces to the identity matrix.
\item The null space of $A$ contains only the zero vector, $\nsp{A}=\set{\zerovector}$.
\item The linear system $\linearsystem{A}{\vect{b}}$ has a unique solution for every possible choice of $\vect{b}$.
\item The columns of $A$ are a linearly independent set.
\item $A$ is invertible.
\item The column space of $A$ is $\complex{n}$, $\csp{A}=\complex{n}$.
\item The columns of $A$ are a basis for $\complex{n}$.
\item The rank of $A$ is $n$, $\rank{A}=n$.
\item The nullity of $A$ is zero, $\nullity{A}=0$.
\end{enumerate}
\end{para}
\end{theorem}
%
\begin{proof}
\begin{para}Building on \acronymref{theorem}{NME5} we can add two of the statements from \acronymref{theorem}{RNNM}.\end{para}
\end{proof}
%
\sageadvice{NME6}{Nonsingular Matrix Equivalences, Round 6}{nonsingular matrix equivalences!round 6}
%
\end{subsect}
%
%  End  d.tex

