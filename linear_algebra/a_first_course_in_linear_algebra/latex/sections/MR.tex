%%%%(c)
%%%%(c)  This file is a portion of the source for the textbook
%%%%(c)
%%%%(c)    A First Course in Linear Algebra
%%%%(c)    Copyright 2004 by Robert A. Beezer
%%%%(c)
%%%%(c)  See the file COPYING.txt for copying conditions
%%%%(c)
%%%%(c)
%%%%%%%%%%%
%%
%%  Section MR
%%  Matrix Representations
%%
%%%%%%%%%%%
%
\begin{introduction}
\begin{para}We have seen that linear transformations whose domain and codomain are vector spaces of columns vectors have a close relationship with matrices (\acronymref{theorem}{MBLT}, \acronymref{theorem}{MLTCV}).  In this section, we will extend the relationship between matrices and linear transformations to the setting of linear transformations between abstract vector spaces.\end{para}
\end{introduction}
%
\begin{subsect}{MR}{Matrix Representations}
%
\begin{para}This is a fundamental definition.\end{para}
%
\begin{definition}{MR}{Matrix Representation}{matrix representation}
\begin{para}Suppose that $\ltdefn{T}{U}{V}$ is a linear transformation, $B=\set{\vectorlist{u}{n}}$ is a basis for $U$ of size $n$, and $C$ is a basis for $V$ of size $m$.  Then the \define{matrix representation} of $T$ relative to $B$ and $C$ is the $m\times n$ matrix,
%
\begin{equation*}
\matrixrep{T}{B}{C}=\left[
\left.\vectrep{C}{\lt{T}{\vect{u}_1}}\right|
\left.\vectrep{C}{\lt{T}{\vect{u}_2}}\right|
\left.\vectrep{C}{\lt{T}{\vect{u}_3}}\right|
\ldots
\left|\vectrep{C}{\lt{T}{\vect{u}_n}}\right.
\right]
\end{equation*}
\end{para}
%
\denote{MR}{Matrix Representation}{$\matrixrep{T}{B}{C}$}{matrix representation}
%
\end{definition}
%
%
\begin{example}{OLTTR}{One linear transformation, three representations}{matrix representations}
\begin{para}Consider the linear transformation
%
\begin{equation*}
\ltdefn{S}{P_3}{M_{22}},\quad \lt{S}{a+bx+cx^2+dx^3}=
\begin{bmatrix}
3a+7b-2c-5d & 8a+14b-2c-11d\\
-4a-8b+2c+6d & 12a+22b-4c-17d
\end{bmatrix}
%
\end{equation*}
\end{para}
%
\begin{para}First, we build a representation relative to the bases,
%
 \begin{align*}
B&=\set{1+2x+x^2-x^3,\,1+3x+x^2+x^3,\,-1-2x+2x^3,\,2+3x+2x^2-5x^3}\\
C&=\set{
\begin{bmatrix}1 & 1 \\ 1 & 2\end{bmatrix},\,
\begin{bmatrix}2 & 3 \\ 2 & 5\end{bmatrix},\,
\begin{bmatrix}-1 & -1 \\ 0 & -2\end{bmatrix},\,
\begin{bmatrix}-1 & -4 \\ -2 & -4\end{bmatrix}
}
\end{align*}
\end{para}
%
\begin{para}We evaluate $S$ with each element of the basis for the domain, $B$, and coordinatize the result relative to the vectors in the basis for the codomain, $C$.  Notice here how we take elements of vector spaces and decompose them into linear combinations of basis elements as the key step in constructing coordinatizations of vectors.  There is a system of equations involved almost every time, but we will omit these details since this should be a routine exercise at this stage.
%
\begin{align*}
&\vectrep{C}{\lt{S}{1+2x+x^2-x^3}}
=\vectrep{C}{\begin{bmatrix}20 & 45 \\ -24 & 69\end{bmatrix}}\\
&\quad\quad=\vectrep{C}{
(-90)\begin{bmatrix}1 & 1 \\ 1 & 2\end{bmatrix}+
37\begin{bmatrix}2 & 3 \\ 2 & 5\end{bmatrix}+
(-40)\begin{bmatrix}-1 & -1 \\ 0 & -2\end{bmatrix}+
4\begin{bmatrix}-1 & -4 \\ -2 & -4\end{bmatrix}
}
=\colvector{-90\\37\\-40\\4}\\
%
&\vectrep{C}{\lt{S}{1+3x+x^2+x^3}}
=\vectrep{C}{\begin{bmatrix}17 & 37 \\ -20 & 57\end{bmatrix}}\\
&\quad\quad=\vectrep{C}{
(-72)\begin{bmatrix}1 & 1 \\ 1 & 2\end{bmatrix}+
29\begin{bmatrix}2 & 3 \\ 2 & 5\end{bmatrix}+
(-34)\begin{bmatrix}-1 & -1 \\ 0 & -2\end{bmatrix}+
3\begin{bmatrix}-1 & -4 \\ -2 & -4\end{bmatrix}
}
=\colvector{-72\\29\\-34\\3}\\
%
&\vectrep{C}{\lt{S}{-1-2x+2x^3}}
=\vectrep{C}{\begin{bmatrix}-27 & -58 \\ 32 & -90\end{bmatrix}}\\
&\quad\quad=\vectrep{C}{
114\begin{bmatrix}1 & 1 \\ 1 & 2\end{bmatrix}+
(-46)\begin{bmatrix}2 & 3 \\ 2 & 5\end{bmatrix}+
54\begin{bmatrix}-1 & -1 \\ 0 & -2\end{bmatrix}+
(-5)\begin{bmatrix}-1 & -4 \\ -2 & -4\end{bmatrix}
}
=\colvector{114\\-46\\54\\-5}\\
%
&\vectrep{C}{\lt{S}{2+3x+2x^2-5x^3}}
=\vectrep{C}{\begin{bmatrix}48 & 109 \\ -58 & 167\end{bmatrix}}\\
&\quad\quad=\vectrep{C}{
(-220)\begin{bmatrix}1 & 1 \\ 1 & 2\end{bmatrix}+
91\begin{bmatrix}2 & 3 \\ 2 & 5\end{bmatrix}+
-96\begin{bmatrix}-1 & -1 \\ 0 & -2\end{bmatrix}+
10\begin{bmatrix}-1 & -4 \\ -2 & -4\end{bmatrix}
}
=\colvector{-220\\91\\-96\\10}\\
%
\end{align*}
\end{para}
%
\begin{para}Thus, employing \acronymref{definition}{MR}
%
\begin{equation*}
\matrixrep{S}{B}{C}=
\begin{bmatrix}
 -90 & -72 & 114 & -220 \\
 37 & 29 & -46 & 91 \\
 -40 & -34 & 54 & -96 \\
 4 & 3 & -5 & 10
\end{bmatrix}
\end{equation*}
\end{para}
%
\begin{para}Often we use ``nice'' bases to build matrix representations and the work involved is much easier.  Suppose we take bases
%
\begin{align*}
D&=\set{1,\,x,\,x^2,\,x^3}
&
E&=\set{
\begin{bmatrix}1&0\\0&0\end{bmatrix},\,
\begin{bmatrix}0&1\\0&0\end{bmatrix},\,
\begin{bmatrix}0&0\\1&0\end{bmatrix},\,
\begin{bmatrix}0&0\\0&1\end{bmatrix}}
%
\end{align*}
\end{para}
%
\begin{para}The evaluation of $S$ at the elements of $D$ is easy and coordinatization relative to $E$ can be done on sight,
%
\begin{align*}
\vectrep{E}{\lt{S}{1}}
&=\vectrep{E}{\begin{bmatrix}3 & 8 \\ -4 & 12\end{bmatrix}}\\
&=\vectrep{E}{
3\begin{bmatrix}1&0\\0&0\end{bmatrix}+
8\begin{bmatrix}0&1\\0&0\end{bmatrix}+
(-4)\begin{bmatrix}0&0\\1&0\end{bmatrix}+
12\begin{bmatrix}0&0\\0&1\end{bmatrix}
}
=\colvector{3 \\ 8 \\ -4 \\ 12}\\
%
\vectrep{E}{\lt{S}{x}}
&=\vectrep{E}{\begin{bmatrix}7 & 14 \\ -8 & 22\end{bmatrix}}\\
&=\vectrep{E}{
7\begin{bmatrix}1&0\\0&0\end{bmatrix}+
14\begin{bmatrix}0&1\\0&0\end{bmatrix}+
(-8)\begin{bmatrix}0&0\\1&0\end{bmatrix}+
22\begin{bmatrix}0&0\\0&1\end{bmatrix}
}
=\colvector{7 \\ 14 \\ -8 \\ 22}\\
%
\vectrep{E}{\lt{S}{x^2}}
&=\vectrep{E}{\begin{bmatrix}-2 & -2 \\ 2 & -4\end{bmatrix}}\\
&=\vectrep{E}{
(-2)\begin{bmatrix}1&0\\0&0\end{bmatrix}+
(-2)\begin{bmatrix}0&1\\0&0\end{bmatrix}+
2\begin{bmatrix}0&0\\1&0\end{bmatrix}+
(-4)\begin{bmatrix}0&0\\0&1\end{bmatrix}
}
=\colvector{-2 \\ -2 \\ 2 \\ -4}\\
%
\vectrep{E}{\lt{S}{x^3}}
&=\vectrep{E}{\begin{bmatrix}-5 & -11 \\ 6 & -17\end{bmatrix}}\\
&=\vectrep{E}{
(-5)\begin{bmatrix}1&0\\0&0\end{bmatrix}+
(-11)\begin{bmatrix}0&1\\0&0\end{bmatrix}+
6\begin{bmatrix}0&0\\1&0\end{bmatrix}+
(-17)\begin{bmatrix}0&0\\0&1\end{bmatrix}
}
=\colvector{-5 \\ -11 \\ 6 \\ -17}\\
%
\end{align*}
\end{para}
%
\begin{para}So the matrix representation of $S$ relative to $D$ and $E$ is
%
\begin{equation*}
\matrixrep{S}{D}{E}=
\begin{bmatrix}
 3 & 7 & -2 & -5 \\
 8 & 14 & -2 & -11 \\
 -4 & -8 & 2 & 6 \\
 12 & 22 & -4 & -17
\end{bmatrix}
\end{equation*}
\end{para}
%
\begin{para}One more time, but now let's use bases
%
\begin{align*}
F&=\set{1+x-x^2+2x^3,\,-1+2x+2x^3,\,2+x-2x^2+3x^3,\,1+x+2x^3}\\
G&=\set{
\begin{bmatrix}1&1\\-1&2\end{bmatrix},\,
\begin{bmatrix}-1&2\\0&2\end{bmatrix},\,
\begin{bmatrix}2&1\\-2&3\end{bmatrix},\,
\begin{bmatrix}1&1\\0&2\end{bmatrix}
}
\end{align*}
%
and evaluate $S$ with the elements of $F$, then coordinatize the results relative to $G$,
%
\begin{align*}
\vectrep{G}{\lt{S}{1+x-x^2+2x^3}}
&=\vectrep{G}{\begin{bmatrix}2 & 2 \\ -2 & 4\end{bmatrix}}
=\vectrep{G}{
2\begin{bmatrix}1&1\\-1&2\end{bmatrix}
}
=\colvector{2\\0\\0\\0}\\
%
\vectrep{G}{\lt{S}{-1+2x+2x^3}}
&=\vectrep{G}{\begin{bmatrix}1 & -2 \\ 0 & -2\end{bmatrix}}
=\vectrep{G}{
(-1)\begin{bmatrix}-1&2\\0&2\end{bmatrix}
}
=\colvector{0\\-1\\0\\0}\\
%
\vectrep{G}{\lt{S}{2+x-2x^2+3x^3}}
&=\vectrep{G}{\begin{bmatrix}2 & 1 \\ -2 & 3\end{bmatrix}}
=\vectrep{G}{
\begin{bmatrix}2&1\\-2&3\end{bmatrix}
}
=\colvector{0\\0\\1\\0}\\
%
\vectrep{G}{\lt{S}{1+x+2x^3}}
&=\vectrep{G}{\begin{bmatrix}0 & 0 \\ 0 & 0\end{bmatrix}}
=\vectrep{G}{
0\begin{bmatrix}1&1\\0&2\end{bmatrix}
}
=\colvector{0\\0\\0\\0}\\
%
\end{align*}
\end{para}
%
\begin{para}So we arrive at an especially economical matrix representation,
%
\begin{equation*}
\matrixrep{S}{F}{G}=
\begin{bmatrix}
2 & 0 & 0 & 0 \\
0 & -1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0
\end{bmatrix}
\end{equation*}
\end{para}
%
\end{example}
%
\begin{para}We may choose to use whatever terms we want when we make a definition.  Some are arbitrary, while others make sense, but only in light of subsequent theorems.  Matrix representation is in the latter category.  We begin with a linear transformation and produce a matrix.  So what?  Here's the theorem that {\em justifies} the term ``matrix representation.''\end{para}
%
\begin{theorem}{FTMR}{Fundamental Theorem of Matrix Representation}{matrix representation}
\begin{para}Suppose that $\ltdefn{T}{U}{V}$ is a linear transformation, $B$ is a basis for $U$, $C$ is a basis for $V$ and $\matrixrep{T}{B}{C}$ is the matrix representation of $T$ relative to $B$ and $C$.  Then, for any $\vect{u}\in U$,
%
\begin{equation*}
\vectrep{C}{\lt{T}{\vect{u}}}=\matrixrep{T}{B}{C}\left(\vectrep{B}{\vect{u}}\right)
\end{equation*}
%
or equivalently
%
\begin{equation*}
\lt{T}{\vect{u}}=\vectrepinv{C}{\matrixrep{T}{B}{C}\left(\vectrep{B}{\vect{u}}\right)}
\end{equation*}
\end{para}
%
\end{theorem}
%
\begin{proof}
\begin{para}Let $B=\set{\vectorlist{u}{n}}$ be the basis of $U$.  Since $\vect{u}\in U$, there are scalars $\scalarlist{a}{n}$ such that
%
\begin{equation*}
\vect{u}=\lincombo{a}{u}{n}
\end{equation*}
\end{para}
%
\begin{para}Then,
%
\begin{align*}
&\matrixrep{T}{B}{C}\vectrep{B}{\vect{u}}\\
&=
\left[
\left.\vectrep{C}{\lt{T}{\vect{u}_1}}\right|
\left.\vectrep{C}{\lt{T}{\vect{u}_2}}\right|
\left.\vectrep{C}{\lt{T}{\vect{u}_3}}\right|
\ldots
\left|\vectrep{C}{\lt{T}{\vect{u}_n}}\right.
\right]
\vectrep{B}{\vect{u}}
&&\text{\acronymref{definition}{MR}}\\
%
&=
\left[
\left.\vectrep{C}{\lt{T}{\vect{u}_1}}\right|
\left.\vectrep{C}{\lt{T}{\vect{u}_2}}\right|
\left.\vectrep{C}{\lt{T}{\vect{u}_3}}\right|
\ldots
\left|\vectrep{C}{\lt{T}{\vect{u}_n}}\right.
\right]
\colvector{a_1\\a_2\\a_3\\\vdots\\a_n}
&&\text{\acronymref{definition}{VR}}\\
%
&=
a_1\vectrep{C}{\lt{T}{\vect{u}_1}}+
a_2\vectrep{C}{\lt{T}{\vect{u}_2}}+
\cdots+
a_n\vectrep{C}{\lt{T}{\vect{u}_n}}
&&\text{\acronymref{definition}{MVP}}\\
%
&=
\vectrep{C}{
a_1\lt{T}{\vect{u}_1}+
a_2\lt{T}{\vect{u}_2}+
a_3\lt{T}{\vect{u}_3}+\cdots+
a_n\lt{T}{\vect{u}_n}
}&&\text{\acronymref{theorem}{LTLC}}\\
%
&=
\vectrep{C}{
\lt{T}{
\lincombo{a}{u}{n}
}}&&\text{\acronymref{theorem}{LTLC}}\\
%
&=
\vectrep{C}{\lt{T}{\vect{u}}}
%
\end{align*}
\end{para}
%
\begin{para}The alternative conclusion is obtained as
%
\begin{align*}
\lt{T}{\vect{u}}
&=\lt{I_V}{\lt{T}{\vect{u}}}
&&\text{\acronymref{definition}{IDLT}}\\
%
&=\lt{\left(\compose{\vectrepinvname{C}}{\vectrepname{C}}\right)}{\lt{T}{\vect{u}}}
&&\text{\acronymref{definition}{IVLT}}\\
%
&=\vectrepinv{C}{\vectrep{C}{\lt{T}{\vect{u}}}}
&&\text{\acronymref{definition}{LTC}}\\
%
&=\vectrepinv{C}{\matrixrep{T}{B}{C}\left(\vectrep{B}{\vect{u}}\right)}\\
\end{align*}
\end{para}
%
\end{proof}
%
\begin{para}This theorem says that we can apply $T$ to $\vect{u}$ and coordinatize the result relative to $C$ in $V$, or we can first coordinatize $\vect{u}$ relative to $B$ in $U$, then multiply by the matrix representation.  Either way, the result is the same.  So the effect of a linear transformation can always be accomplished by a matrix-vector product (\acronymref{definition}{MVP}).  That's important enough to say again.  The effect of a linear transformation is a matrix-vector product.
%
\diagram{FTMR}{Fundamental Theorem of Matrix Representations}
%
\begin{graphics}{FTMR}{Fundamental Theorem of Matrix Representations}
\matrix (m) [matrix of math nodes, row sep=5em, column sep=10em, text height=1.5ex, text depth=0.25ex]
{ \vect{u} & \lt{T}{\vect{u}} \\
\vectrep{B}{\vect{u}} & \matrixrep{T}{B}{C}\,\vectrep{B}{\vect{u}}=\vectrep{C}{\lt{T}{\vect{u}}}\\};
\path[->]
(m-1-1) edge[thick] node[auto] {$T$}                   (m-1-2)
(m-1-2) edge[thick] node[auto] {$\vectrepname{C}$}     (m-2-2)
(m-1-1) edge[thick] node[auto] {$\vectrepname{B}$}     (m-2-1)
(m-2-1) edge[thick] node[auto] {$\matrixrep{T}{B}{C}$} (m-2-2);
\end{graphics}
%
\end{para}
%
\begin{para}The alternative conclusion of this result might be even more striking.  It says that to effect a linear transformation ($T$) of a vector ($\vect{u}$), coordinatize the input (with $\vectrepname{B}$), do a matrix-vector product (with $\matrixrep{T}{B}{C}$), and un-coordinatize the result (with $\vectrepinvname{C}$).  So, absent some bookkeeping about vector representations, a linear transformation {\em is} a matrix.  To adjust the diagram, we ``reverse'' the arrow on the right, which means inverting the vector representation $\vectrepname{C}$ on $V$.  Now we can go directly across the top of the diagram, computing the linear transformation between the abstract vector spaces.  Or, we can around the other three sides, using vector representation, a matrix-vector product, followed by un-coordinatization.
%
\diagram{FTMRA}{Fundamental Theorem of Matrix Representations (Alternate)}
%
\begin{graphics}{FTMRA}{Fundamental Theorem of Matrix Representations (Alternate)}
\matrix (m) [matrix of math nodes, row sep=5em, column sep=10em, text height=1.5ex, text depth=0.25ex]
{\vect{u} & \lt{T}{\vect{u}}=\vectrepinv{C}{\matrixrep{T}{B}{C}\,\vectrep{B}{\vect{u}}}\\
\vectrep{B}{\vect{u}} & \matrixrep{T}{B}{C}\,\vectrep{B}{\vect{u}}\\};
\path[->]
(m-1-1) edge[thick] node[auto] {$T$}                   (m-1-2)
(m-2-2) edge[thick] node[auto] {$\vectrepinvname{C}$}  (m-1-2) % reversed direction
(m-1-1) edge[thick] node[auto] {$\vectrepname{B}$}     (m-2-1)
(m-2-1) edge[thick] node[auto] {$\matrixrep{T}{B}{C}$} (m-2-2);
\end{graphics}
%
\end{para}
%
\begin{para}Here's an example to illustrate how the ``action'' of a linear transformation can be effected by matrix multiplication.\end{para}
%
\begin{example}{ALTMM}{A linear transformation as matrix multiplication}{linear transformation!as matrix multiplication}
\begin{para}In \acronymref{example}{OLTTR} we found three representations of the linear transformation $S$.  In this example, we will compute a single output of $S$ in four different ways.  First ``normally,'' then three times over using \acronymref{theorem}{FTMR}.\end{para}
%
\begin{para}Choose $p(x)=3-x+2x^2-5x^3$, for no particular reason.  Then the straightforward application of $S$ to $p(x)$ yields
%
\begin{align*}
\lt{S}{p(x)}
&=\lt{S}{3-x+2x^2-5x^3}\\
&=
\begin{bmatrix}
3(3)+7(-1)-2(2)-5(-5) & 8(3)+14(-1)-2(2)-11(-5)\\
-4(3)-8(-1)+2(2)+6(-5) & 12(3)+22(-1)-4(2)-17(-5)
\end{bmatrix}\\
&=
\begin{bmatrix}
23 & 61 \\ -30 & 91
\end{bmatrix}
%
\end{align*}
\end{para}
%
\begin{para}Now use the representation of $S$ relative to the bases $B$ and $C$ and \acronymref{theorem}{FTMR}.  Note that we will employ the following linear combination in moving from the second line to the third,
%
\begin{align*}
3-x+2x^2-5x^3
&= 48(1+2x+x^2-x^3)+(-20)(1+3x+x^2+x^3)+\\
&\quad\quad (-1)(-1-2x+2x^3)+(-13)(2+3x+2x^2-5x^3)
\end{align*}
\end{para}
%
\begin{para}\begin{align*}
\lt{S}{p(x)}
&=\vectrepinv{C}{\matrixrep{S}{B}{C}\vectrep{B}{p(x)}}\\
%
&=\vectrepinv{C}{\matrixrep{S}{B}{C}\vectrep{B}{3-x+2x^2-5x^3}}\\
%
&=\vectrepinv{C}{\matrixrep{S}{B}{C}\colvector{48\\-20\\-1\\-13}}\\
%
&=\vectrepinv{C}{
\begin{bmatrix}
 -90 & -72 & 114 & -220 \\
 37 & 29 & -46 & 91 \\
 -40 & -34 & 54 & -96 \\
 4 & 3 & -5 & 10
\end{bmatrix}
\colvector{48\\-20\\-1\\-13}}\\
%
&=\vectrepinv{C}{\colvector{-134\\59\\-46\\7}}\\
%
&=
(-134)\begin{bmatrix}1 & 1 \\ 1 & 2\end{bmatrix}+
59\begin{bmatrix}2 & 3 \\ 2 & 5\end{bmatrix}+
(-46)\begin{bmatrix}-1 & -1 \\ 0 & -2\end{bmatrix}+
7\begin{bmatrix}-1 & -4 \\ -2 & -4\end{bmatrix}\\
%
&=
\begin{bmatrix}
23 & 61 \\ -30 & 91
\end{bmatrix}
\end{align*}
\end{para}
%
\begin{para}Again, but now with ``nice'' bases like $D$ and $E$, and the computations are more transparent.
\begin{align*}
\lt{S}{p(x)}
&=\vectrepinv{E}{\matrixrep{S}{D}{E}\vectrep{D}{p(x)}}\\
%
&=\vectrepinv{E}{\matrixrep{S}{D}{E}\vectrep{D}{3-x+2x^2-5x^3}}\\
%
&=\vectrepinv{E}{\matrixrep{S}{D}{E}\vectrep{D}{3(1)+(-1)(x)+2(x^2)+(-5)(x^3)}}\\
%
&=\vectrepinv{E}{\matrixrep{S}{D}{E}\colvector{3\\-1\\2\\-5}}\\
%
&=\vectrepinv{E}{
\begin{bmatrix}
 3 & 7 & -2 & -5 \\
 8 & 14 & -2 & -11 \\
 -4 & -8 & 2 & 6 \\
 12 & 22 & -4 & -17
\end{bmatrix}
\colvector{3\\-1\\2\\-5}
}\\
%
&=\vectrepinv{E}{\colvector{23 \\ 61 \\ -30 \\ 91}}\\
%
&=
23\begin{bmatrix}1&0\\0&0\end{bmatrix}+
61\begin{bmatrix}0&1\\0&0\end{bmatrix}+
(-30)\begin{bmatrix}0&0\\1&0\end{bmatrix}+
91\begin{bmatrix}0&0\\0&1\end{bmatrix}\\
%
&=
\begin{bmatrix}
23 & 61 \\ -30 & 91
\end{bmatrix}
\end{align*}
\end{para}
%
\begin{para}OK, last time, now with the bases $F$ and $G$.  The coordinatizations will take some work this time, but the matrix-vector product (\acronymref{definition}{MVP}) (which is the actual action of the linear transformation) will be especially easy, given the diagonal nature of the matrix representation, $\matrixrep{S}{F}{G}$.  Here we go,
\begin{align*}
\lt{S}{p(x)}
&=\vectrepinv{G}{\matrixrep{S}{F}{G}\vectrep{F}{p(x)}}\\
%
&=\vectrepinv{G}{\matrixrep{S}{F}{G}\vectrep{F}{3-x+2x^2-5x^3}}\\
%
&=\vectrepinv{G}{\matrixrep{S}{F}{G}\vectrep{F}{
32(1+x-x^2+2x^3)
-7(-1+2x+2x^3)
-17(2+x-2x^2+3x^3)
-2(1+x+2x^3)
}}\\
%
&=\vectrepinv{G}{\matrixrep{S}{F}{G}\colvector{32\\-7\\-17\\-2}}\\
%
&=\vectrepinv{G}{
\begin{bmatrix}
2 & 0 & 0 & 0 \\
0 & -1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0
\end{bmatrix}
\colvector{32\\-7\\-17\\-2}
}\\
%
&=\vectrepinv{G}{\colvector{64 \\ 7 \\ -17 \\ 0}}\\
%
&=
64\begin{bmatrix}1&1\\-1&2\end{bmatrix}+
7\begin{bmatrix}-1&2\\0&2\end{bmatrix}+
(-17)\begin{bmatrix}2&1\\-2&3\end{bmatrix}+
0\begin{bmatrix}1&1\\0&2\end{bmatrix}\\
%
&=
\begin{bmatrix}
23 & 61 \\ -30 & 91
\end{bmatrix}
\end{align*}
\end{para}
%
\begin{para}This example is not meant to necessarily illustrate that any one of these four computations is simpler than the others.  Instead, it is meant to illustrate the many different ways we can arrive at the same result, with the last three all employing a matrix representation to effect the linear transformation.\end{para}
%
\end{example}
%
\begin{para}We will use \acronymref{theorem}{FTMR} frequently in the next few sections.  A typical application will feel like the linear transformation $T$ ``commutes'' with a vector representation, $\vectrepname{C}$, and as it does the transformation morphs into a matrix, $\matrixrep{T}{B}{C}$, while the vector representation changes to a new basis, $\vectrepname{B}$.  Or vice-versa.\end{para}
%
\sageadvice{MR}{Matrix Representations}{matrix representations}
%
\end{subsect}
%
\begin{subsect}{NRFO}{New Representations from Old}
%
\begin{para}In \acronymref{subsection}{LT.NLTFO} we built new linear transformations from other linear transformations.  Sums, scalar multiples and compositions.  These new linear transformations will have matrix representations as well.  How do the new matrix representations relate to the old matrix representations?  Here are the three theorems.\end{para}
%
\begin{theorem}{MRSLT}{Matrix Representation of a Sum of Linear Transformations}{matrix representation!sum of linear transformations}
\begin{para}Suppose that $\ltdefn{T}{U}{V}$ and $\ltdefn{S}{U}{V}$ are linear transformations, $B$ is a basis of $U$ and $C$ is a basis of $V$.  Then
%
\begin{equation*}
\matrixrep{T+S}{B}{C}=\matrixrep{T}{B}{C}+\matrixrep{S}{B}{C}
\end{equation*}
\end{para}
%
\end{theorem}
%
\begin{proof}
\begin{para}Let $\vect{x}$ be any vector in $\complex{n}$.  Define $\vect{u}\in U$ by $\vect{u}=\vectrepinv{B}{\vect{x}}$, so $\vect{x}=\vectrep{B}{\vect{u}}$.  Then,
%
\begin{align*}
\matrixrep{T+S}{B}{C}\vect{x}&=
\matrixrep{T+S}{B}{C}\vectrep{B}{\vect{u}}&&\text{Substitution}\\
&=\vectrep{C}{\lt{\left(T+S\right)}{\vect{u}}}&&\text{\acronymref{theorem}{FTMR}}\\
&=\vectrep{C}{\lt{T}{\vect{u}}+\lt{S}{\vect{u}}}&&\text{\acronymref{definition}{LTA}}\\
&=\vectrep{C}{\lt{T}{\vect{u}}}+\vectrep{C}{\lt{S}{\vect{u}}}&&\text{\acronymref{definition}{LT}}\\
&=\matrixrep{T}{B}{C}\left(\vectrep{B}{\vect{u}}\right)+\matrixrep{S}{B}{C}\left(\vectrep{B}{\vect{u}}\right)&&\text{\acronymref{theorem}{FTMR}}\\
&=\left(\matrixrep{T}{B}{C}+\matrixrep{S}{B}{C}\right)\vectrep{B}{\vect{u}}&&\text{\acronymref{theorem}{MMDAA}}\\
&=\left(\matrixrep{T}{B}{C}+\matrixrep{S}{B}{C}\right)\vect{x}&&\text{Substitution}
\end{align*}
\end{para}
%
\begin{para}Since the matrices $\matrixrep{T+S}{B}{C}$ and $\matrixrep{T}{B}{C}+\matrixrep{S}{B}{C}$ have equal matrix-vector products for {\em every} vector in $\complex{n}$, by \acronymref{theorem}{EMMVP} they are equal matrices.  (Now would be a good time to double-back and study the proof of \acronymref{theorem}{EMMVP}.  You did promise to come back to this theorem sometime, didn't you?)\end{para}
%
\end{proof}
%
%
\begin{theorem}{MRMLT}{Matrix Representation of a Multiple of a Linear Transformation}{matrix representation!multiple of a linear transformation}
\begin{para}Suppose that $\ltdefn{T}{U}{V}$ is a linear transformation, $\alpha\in\complex{\null}$, $B$ is a basis of $U$ and $C$ is a basis of $V$.  Then
%
\begin{equation*}
\matrixrep{\alpha T}{B}{C}=\alpha\matrixrep{T}{B}{C}
\end{equation*}
\end{para}
%
\end{theorem}
%
\begin{proof}
\begin{para}Let $\vect{x}$ be any vector in $\complex{n}$.  Define $\vect{u}\in U$ by $\vect{u}=\vectrepinv{B}{\vect{x}}$, so $\vect{x}=\vectrep{B}{\vect{u}}$.  Then,
%
\begin{align*}
\matrixrep{\alpha T}{B}{C}\vect{x}&=
\matrixrep{\alpha T}{B}{C}\vectrep{B}{\vect{u}}&&\text{Substitution}\\
&=\vectrep{C}{\lt{\left(\alpha T\right)}{\vect{u}}}&&\text{\acronymref{theorem}{FTMR}}\\
&=\vectrep{C}{\alpha\lt{T}{\vect{u}}}&&\text{\acronymref{definition}{LTSM}}\\
&=\alpha\vectrep{C}{\lt{T}{\vect{u}}}&&\text{\acronymref{definition}{LT}}\\
&=\alpha\left(\matrixrep{T}{B}{C}\vectrep{B}{\vect{u}}\right)&&\text{\acronymref{theorem}{FTMR}}\\
&=\left(\alpha\matrixrep{T}{B}{C}\right)\vectrep{B}{\vect{u}}&&\text{\acronymref{theorem}{MMSMM}}\\
&=\left(\alpha\matrixrep{T}{B}{C}\right)\vect{x}&&\text{Substitution}
\end{align*}
\end{para}
%
\begin{para}Since the matrices $\matrixrep{\alpha T}{B}{C}$ and $\alpha\matrixrep{T}{B}{C}$ have equal matrix-vector products for {\em every} vector in $\complex{n}$,  by \acronymref{theorem}{EMMVP} they are equal matrices.\end{para}
%
\end{proof}
%
\begin{para}The vector space of all linear transformations from $U$ to $V$ is now isomorphic to the vector space of all $m\times n$ matrices.\end{para}
%
\begin{theorem}{MRCLT}{Matrix Representation of a Composition of Linear Transformations}{matrix representation!composition of linear transformations}
\begin{para}Suppose that $\ltdefn{T}{U}{V}$ and $\ltdefn{S}{V}{W}$ are linear transformations, $B$ is a basis of $U$, $C$ is a basis of $V$, and $D$ is a basis of $W$.  Then
%
\begin{equation*}
\matrixrep{\compose{S}{T}}{B}{D}=\matrixrep{S}{C}{D}\matrixrep{T}{B}{C}
\end{equation*}
\end{para}
%
\end{theorem}
%
\begin{proof}
\begin{para}Let $\vect{x}$ be any vector in $\complex{n}$.  Define $\vect{u}\in U$ by $\vect{u}=\vectrepinv{B}{\vect{x}}$, so $\vect{x}=\vectrep{B}{\vect{u}}$.  Then,
%
\begin{align*}
\matrixrep{\compose{S}{T}}{B}{D}\vect{x}&=
\matrixrep{\compose{S}{T}}{B}{D}\vectrep{B}{\vect{u}}&&\text{Substitution}\\
&=\vectrep{D}{\lt{\left(\compose{S}{T}\right)}{\vect{u}}}&&\text{\acronymref{theorem}{FTMR}}\\
&=\vectrep{D}{\lt{S}{\lt{T}{\vect{u}}}}&&\text{\acronymref{definition}{LTC}}\\
&=\matrixrep{S}{C}{D}\vectrep{C}{\lt{T}{\vect{u}}}&&\text{\acronymref{theorem}{FTMR}}\\
&=\matrixrep{S}{C}{D}\left(\matrixrep{T}{B}{C}\vectrep{B}{\vect{u}}\right)&&\text{\acronymref{theorem}{FTMR}}\\
&=\left(\matrixrep{S}{C}{D}\matrixrep{T}{B}{C}\right)\vectrep{B}{\vect{u}}&&\text{\acronymref{theorem}{MMA}}\\
&=\left(\matrixrep{S}{C}{D}\matrixrep{T}{B}{C}\right)\vect{x}&&\text{Substitution}
\end{align*}
\end{para}
%
\begin{para}Since the matrices $\matrixrep{\compose{S}{T}}{B}{D}$ and $\matrixrep{S}{C}{D}\matrixrep{T}{B}{C}$ have equal matrix-vector products for {\em every} vector in $\complex{n}$, by \acronymref{theorem}{EMMVP} they are equal matrices.\end{para}
%
\end{proof}
%
\begin{para}This is the second great surprise of introductory linear algebra.  Matrices are linear transformations (functions, really), and matrix multiplication is function composition!  We can form the composition of two linear transformations, then form the matrix representation of the result.  Or we can form the matrix representation of each linear transformation separately, then {\em multiply} the two representations together via \acronymref{definition}{MM}.  In either case, we arrive at the same result.\end{para}
%
\begin{example}{MPMR}{Matrix product of matrix representations}{matrix product!as composition of linear transformations}
\begin{para}Consider the two linear transformations,
%
\begin{align*}
\ltdefn{T}{\complex{2}}{P_2}&\quad
\lt{T}{\colvector{a\\b}}=(-a + 3b)+(2a + 4b)x+(a - 2b)x^2\\
%
\ltdefn{S}{P_2}{M_{22}}&\quad\
\lt{S}{a+bx+cx^2}=
\begin{bmatrix}
2a + b + 2c & a + 4b - c\\
-a + 3c & 3a + b + 2 c\end{bmatrix}
\end{align*}
%
and bases for $\complex{2}$, $P_2$ and $M_{22}$ (respectively),
%
\begin{align*}
B&=\set{\colvector{3\\1},\,\colvector{2\\1}}\\
C&=\set{1-2x+x^2,\,-1+3x,\,2x+3x^2}\\
D&=\set{
\begin{bmatrix} 1 & -2\\ 1 & -1 \end{bmatrix},\,
\begin{bmatrix} 1 & -1\\ 1 & 2\end{bmatrix},\,
\begin{bmatrix} -1 & 2 \\ 0 & 0 \end{bmatrix},\,
\begin{bmatrix} 2 & -3 \\ 2 & 2\end{bmatrix}
}
\end{align*}
\end{para}
%
\begin{para}Begin by computing the new linear transformation that is the composition of $T$ and $S$ (\acronymref{definition}{LTC}, \acronymref{theorem}{CLTLT}),  $\ltdefn{\left(\compose{S}{T}\right)}{\complex{2}}{M_{22}}$,
%
\begin{align*}
\lt{\left(\compose{S}{T}\right)}{\colvector{a\\b}}
&=\lt{S}{\lt{T}{\colvector{a\\b}}}\\
&=\lt{S}{(-a + 3b)+(2a + 4b)x+(a - 2b)x^2}\\
&=
\begin{bmatrix}
2(-a + 3b) + (2a + 4b) + 2(a - 2b) & (-a + 3b) + 4(2a + 4b) - (a - 2b)\\
-(-a + 3b) + 3(a - 2b) & 3(-a + 3b) + (2a + 4b) + 2(a - 2b)
\end{bmatrix}\\
&=
\begin{bmatrix}
2a + 6b & 6a + 21b\\
4a - 9b &a + 9b
\end{bmatrix}
\end{align*}
\end{para}
%
\begin{para}Now compute the matrix representations (\acronymref{definition}{MR}) for each of these three linear transformations ($T$, $S$, $\compose{S}{T}$), relative to the appropriate bases.  First for $T$,
%
\begin{align*}
\vectrep{C}{\lt{T}{\colvector{3\\1}}}&=\vectrep{C}{10x+x^2}\\
&=\vectrep{C}{28(1-2x+x^2)+28(-1+3x)+(-9)(2x+3x^2)}=\colvector{28\\28\\-9}\\
%
\vectrep{C}{\lt{T}{\colvector{2\\1}}}&=\vectrep{C}{1+8x}\\
&=\vectrep{C}{33(1-2x+x^2)+32(-1+3x)+(-11)(2x+3x^2)}=\colvector{33\\32\\-11}\\
%
\end{align*}
\end{para}
%
\begin{para}So we have the matrix representation of $T$,
%
\begin{equation*}
\matrixrep{T}{B}{C}=
\begin{bmatrix}
 28 & 33 \\
 28 & 32 \\
 -9 & -11
\end{bmatrix}
\end{equation*}
\end{para}
%
\begin{para}Now, a representation of $S$,
\begin{align*}
\vectrep{D}{\lt{S}{1-2x+x^2}}&=\vectrep{D}{\begin{bmatrix}2 & -8 \\ 2 & 3 \end{bmatrix}}\\
&=\vectrep{D}{
(-11)\begin{bmatrix} 1 & -2\\ 1 & -1 \end{bmatrix}+
(-21)\begin{bmatrix} 1 & -1\\ 1 & 2\end{bmatrix}+
0\begin{bmatrix} -1 & 2 \\ 0 & 0 \end{bmatrix}+
(17)\begin{bmatrix} 2 & -3 \\ 2 & 2\end{bmatrix}
}\\
&=\colvector{-11\\-21\\0\\17}\\
%
\vectrep{D}{\lt{S}{-1+3x}}&=\vectrep{D}{\begin{bmatrix}1 & 11 \\ 1 & 0\end{bmatrix}}\\
&=\vectrep{D}{
26\begin{bmatrix} 1 & -2\\ 1 & -1 \end{bmatrix}+
51\begin{bmatrix} 1 & -1\\ 1 & 2\end{bmatrix}+
0\begin{bmatrix} -1 & 2 \\ 0 & 0 \end{bmatrix}+
(-38)\begin{bmatrix} 2 & -3 \\ 2 & 2\end{bmatrix}
}\\
&=\colvector{26\\51\\0\\-38}\\
%
\vectrep{D}{\lt{S}{2x+3x^2}}&=\vectrep{D}{\begin{bmatrix}8 & 5 \\ 9 & 8\end{bmatrix}}\\
&=\vectrep{D}{
34\begin{bmatrix} 1 & -2\\ 1 & -1 \end{bmatrix}+
67\begin{bmatrix} 1 & -1\\ 1 & 2\end{bmatrix}+
1\begin{bmatrix} -1 & 2 \\ 0 & 0 \end{bmatrix}+
(-46)\begin{bmatrix} 2 & -3 \\ 2 & 2\end{bmatrix}
}\\
&=\colvector{34\\67\\1\\-46}\\
%
\end{align*}
\end{para}
%
\begin{para}So we have the matrix representation of $S$,
%
\begin{equation*}
\matrixrep{S}{C}{D}=
\begin{bmatrix}
 -11 & 26 & 34 \\
 -21 & 51 & 67 \\
 0 & 0 & 1 \\
 17 & -38 & -46
\end{bmatrix}
\end{equation*}
\end{para}
%
\begin{para}Finally, a representation of $\compose{S}{T}$,
%
\begin{align*}
\vectrep{D}{\lt{\left(\compose{S}{T}\right)}{\colvector{3\\1}}}
&=\vectrep{D}{\begin{bmatrix}12 & 39\\3 &12\end{bmatrix}}\\
&=\vectrep{D}{
114\begin{bmatrix} 1 & -2\\ 1 & -1 \end{bmatrix}+
237\begin{bmatrix} 1 & -1\\ 1 & 2\end{bmatrix}+
(-9)\begin{bmatrix} -1 & 2 \\ 0 & 0 \end{bmatrix}+
(-174)\begin{bmatrix} 2 & -3 \\ 2 & 2\end{bmatrix}
}\\
&=\colvector{114\\237\\-9\\-174}\\
%
\vectrep{D}{\lt{\left(\compose{S}{T}\right)}{\colvector{2\\1}}}
&=\vectrep{D}{\begin{bmatrix}10 & 33\\-1 & 11\end{bmatrix}}\\
&=\vectrep{D}{
95\begin{bmatrix} 1 & -2\\ 1 & -1 \end{bmatrix}+
202\begin{bmatrix} 1 & -1\\ 1 & 2\end{bmatrix}+
(-11)\begin{bmatrix} -1 & 2 \\ 0 & 0 \end{bmatrix}+
(-149)\begin{bmatrix} 2 & -3 \\ 2 & 2\end{bmatrix}
}\\
&=\colvector{95\\202\\-11\\-149}\\
%
\end{align*}
\end{para}
%
\begin{para}So we have the matrix representation of $\compose{S}{T}$,
%
\begin{equation*}
\matrixrep{\compose{S}{T}}{B}{D}=
\begin{bmatrix}
 114 & 95 \\
 237 & 202 \\
 -9 & -11 \\
 -174 & -149
\end{bmatrix}
\end{equation*}
\end{para}
%
\begin{para}Now, we are all set to verify the conclusion of \acronymref{theorem}{MRCLT},
%
\begin{align*}
\matrixrep{S}{C}{D}\matrixrep{T}{B}{C}
&=
\begin{bmatrix}
 -11 & 26 & 34 \\
 -21 & 51 & 67 \\
 0 & 0 & 1 \\
 17 & -38 & -46
\end{bmatrix}
%
\begin{bmatrix}
 28 & 33 \\
 28 & 32 \\
 -9 & -11
\end{bmatrix}\\
&=
\begin{bmatrix}
 114 & 95 \\
 237 & 202 \\
 -9 & -11 \\
 -174 & -149
\end{bmatrix}\\
&=\matrixrep{\compose{S}{T}}{B}{D}
\end{align*}
\end{para}
%
\begin{para}We have intentionally used non-standard bases.  If you were to choose ``nice'' bases for the three vector spaces, then the result of the theorem might be rather transparent.  But this would still be a worthwhile exercise --- give it a go.\end{para}
%
\end{example}
%
\begin{para}A diagram, similar to ones we have seen earlier, might make the importance of this theorem clearer,
%
\diagram{MRCLT}{Matrix Representation and Composition of Linear Transformations}
%
\begin{graphics}{MRCLT}{Matrix Representation and Composition of Linear Transformations}
\matrix (m) [matrix of math nodes, row sep=5em, column sep=10em, text height=1.5ex, text depth=0.25ex]
{ S, T & \matrixrep{S}{C}{D},\ \matrixrep{T}{B}{C} \\
\compose{S}{T} & \matrixrep{\compose{S}{T}}{B}{D}=\matrixrep{S}{C}{D}\,\matrixrep{T}{B}{C}\\};
\path[->]
(m-1-1) edge[thick] node[auto]  {Definition MR}  (m-1-2)
(m-1-2) edge[thick] node[right] {Definition MM}  (m-2-2)
(m-1-1) edge[thick] node[left]  {Definition LTC} (m-2-1)
(m-2-1) edge[thick] node[auto]  {Definition MR}  (m-2-2);
\end{graphics}
\end{para}
%
\begin{para}One of our goals in the first part of this book is to make the definition of matrix multiplication (\acronymref{definition}{MVP}, \acronymref{definition}{MM}) seem as natural as possible.  However, many of us are brought up with an entry-by-entry description of matrix multiplication (\acronymref{theorem}{EMP}) as the {\em definition} of matrix multiplication, and then theorems about columns of matrices and linear combinations follow from that definition.  With this unmotivated definition, the realization that matrix multiplication is function composition is quite remarkable.  It is an interesting exercise to begin with the question, ``What is the matrix representation of the composition of two linear transformations?'' and then, without using any theorems about matrix multiplication, finally arrive at the entry-by-entry description of matrix multiplication.  Try it yourself (\acronymref{exercise}{MR.T80}).\end{para}
%
\sageadvice{SUTH3}{Sage Under The Hood, Round 3}{sage under the hood!round 3}
%
\end{subsect}
%
\begin{subsect}{PMR}{Properties of Matrix Representations}
%
\begin{para}It will not be a surprise to discover that the kernel and range of a linear transformation are closely related to the null space and column space of the transformation's matrix representation.  Perhaps this idea has been bouncing around in your head already, even before seeing the definition of a matrix representation.  However, with a formal definition of a matrix representation (\acronymref{definition}{MR}), and a fundamental theorem to go with it (\acronymref{theorem}{FTMR}) we can be formal about the relationship, using the idea of isomorphic vector spaces (\acronymref{definition}{IVS}).  Here are the twin theorems.\end{para}
%
\begin{theorem}{KNSI}{Kernel and Null Space Isomorphism}{kernel!isomorphic to null space}
\index{null space!isomorphic to kernel}
\begin{para}Suppose that $\ltdefn{T}{U}{V}$ is a linear transformation, $B$ is a basis for $U$ of size $n$, and $C$ is a basis for $V$.  Then the kernel of $T$ is isomorphic to the null space of $\matrixrep{T}{B}{C}$,
%
\begin{equation*}
\krn{T}\isomorphic\nsp{\matrixrep{T}{B}{C}}
\end{equation*}
\end{para}
%
\end{theorem}
%
\begin{proof}
\begin{para}To establish that two vector spaces are isomorphic, we must find an isomorphism between them, an invertible linear transformation (\acronymref{definition}{IVS}).  The kernel of the linear transformation $T$, $\krn{T}$, is a subspace of $U$, while the null space of the matrix representation, $\nsp{\matrixrep{T}{B}{C}}$ is a subspace of $\complex{n}$.  The function $\vectrepname{B}$ is defined as a function from $U$ to $\complex{n}$, but we can just as well employ the definition of $\vectrepname{B}$ as a function from $\krn{T}$ to $\nsp{\matrixrep{T}{B}{C}}$.\end{para}
%
\begin{para}We must first insure that if we choose an input for $\vectrepname{B}$ from $\krn{T}$ that then the output will be an element of $\nsp{\matrixrep{T}{B}{C}}$.  So suppose that $\vect{u}\in\krn{T}$.  Then
%
\begin{align*}
\matrixrep{T}{B}{C}\vectrep{B}{\vect{u}}
&=\vectrep{C}{\lt{T}{\vect{u}}}&&\text{\acronymref{theorem}{FTMR}}\\
%
&=\vectrep{C}{\zerovector}&&\text{\acronymref{definition}{KLT}}\\
%
&=\zerovector&&\text{\acronymref{theorem}{LTTZZ}}\\
%
\end{align*}
\end{para}
%
\begin{para}This says that $\vectrep{B}{\vect{u}}\in\nsp{\matrixrep{T}{B}{C}}$, as desired.\end{para}
%
\begin{para}The restriction in the size of the domain and codomain $\vectrepname{B}$ will not affect the fact that $\vectrepname{B}$ is a linear transformation (\acronymref{theorem}{VRLT}), nor will it affect the fact that $\vectrepname{B}$ is injective (\acronymref{theorem}{VRI}).  Something must be done though to verify that $\vectrepname{B}$ is surjective.  To this end, appeal to the definition of surjective (\acronymref{definition}{SLT}), and suppose that we have an element of the codomain, $\vect{x}\in\nsp{\matrixrep{T}{B}{C}}\subseteq\complex{n}$ and we wish to find an element of the domain with $\vect{x}$ as its image.  We now show that the desired element of the domain is $\vect{u}=\vectrepinv{B}{\vect{x}}$.  First, verify that $\vect{u}\in\krn{T}$,
%
\begin{align*}
\lt{T}{\vect{u}}
&=\lt{T}{\vectrepinv{B}{\vect{x}}}\\
&=\vectrepinv{C}{\matrixrep{T}{B}{C}\left(\vectrep{B}{\vectrepinv{B}{\vect{x}}}\right)}&&\text{\acronymref{theorem}{FTMR}}\\
&=\vectrepinv{C}{\matrixrep{T}{B}{C}\left(\lt{I_{\complex{n}}}{\vect{x}}\right)}&&\text{\acronymref{definition}{IVLT}}\\
&=\vectrepinv{C}{\matrixrep{T}{B}{C}\vect{x}}&&\text{\acronymref{definition}{IDLT}}\\
&=\vectrepinv{C}{\zerovector_{\complex{n}}}&&\text{\acronymref{definition}{KLT}}\\
&=\zerovector_{V}&&\text{\acronymref{theorem}{LTTZZ}}\\
%
\intertext{Second, verify that the proposed isomorphism, $\vectrepname{B}$, takes $\vect{u}$ to $\vect{x}$,}
%
\vectrep{B}{\vect{u}}&=\vectrep{B}{\vectrepinv{B}{\vect{x}}}&&\text{Substitution}\\
&=\lt{I_{\complex{n}}}{\vect{x}}&&\text{\acronymref{definition}{IVLT}}\\
&=\vect{x}&&\text{\acronymref{definition}{IDLT}}
%
\end{align*}
\end{para}
%
\begin{para}With $\vectrepname{B}$ demonstrated to be an injective and surjective linear transformation from $\krn{T}$ to $\nsp{\matrixrep{T}{B}{C}}$, \acronymref{theorem}{ILTIS} tells us $\vectrepname{B}$ is invertible, and so by \acronymref{definition}{IVS}, we say $\krn{T}$ and $\nsp{\matrixrep{T}{B}{C}}$ are isomorphic.\end{para}
%
\end{proof}
%
\begin{example}{KVMR}{Kernel via matrix representation}{kernel!via matrix representation}
\begin{para}Consider the kernel of the linear transformation
%
\begin{equation*}
\ltdefn{T}{M_{22}}{P_2},\quad \lt{T}{\begin{bmatrix}a&b\\c&d\end{bmatrix}}=
(2a-b+c-5d)+(a+4b+5c+2d)x+(3a-2b+c-8d)x^2
\end{equation*}
\end{para}
%
\begin{para}We will begin with a matrix representation of $T$ relative to the bases for $M_{22}$ and $P_2$ (respectively),
%
\begin{align*}
B&=\set{
\begin{bmatrix}1 & 2 \\ -1 & -1\end{bmatrix},\,
\begin{bmatrix}1 & 3 \\ -1 & -4\end{bmatrix},\,
\begin{bmatrix}1 & 2 \\ 0 & -2\end{bmatrix},\,
\begin{bmatrix}2 & 5 \\ -2 & -4\end{bmatrix}
}\\
%
C&=\set{1+x+x^2,\,2+3x,\,-1-2x^2}
\end{align*}
\end{para}
%
\begin{para}Then,
%
\begin{align*}
\vectrep{C}{\lt{T}{\begin{bmatrix}1 & 2 \\ -1 & -1\end{bmatrix}}}
&=\vectrep{C}{4+2x+6x^2}\\
&=\vectrep{C}{2(1+x+x^2)+0(2+3x)+(-2)(-1-2x^2)}\\
&=\colvector{2\\0\\-2}\\
%
\vectrep{C}{\lt{T}{\begin{bmatrix}1 & 3 \\ -1 & -4\end{bmatrix}}}
&=\vectrep{C}{18+28x^2}\\
&=\vectrep{C}{(-24)(1+x+x^2)+8(2+3x)+(-26)(-1-2x^2)}\\
&=\colvector{-24\\8\\-26}\\
%
\vectrep{C}{\lt{T}{\begin{bmatrix}1 & 2 \\ 0 & -2\end{bmatrix}}}
&=\vectrep{C}{10+5x+15x^2}\\
&=\vectrep{C}{5(1+x+x^2)+0(2+3x)+(-5)(-1-2x^2)}\\
&=\colvector{5\\0\\-5}\\
%
\vectrep{C}{\lt{T}{\begin{bmatrix}2 & 5 \\ -2 & -4\end{bmatrix}}}
&=\vectrep{C}{17+4x+26x^2}\\
&=\vectrep{C}{(-8)(1+x+x^2)+(4)(2+3x)+(-17)(-1-2x^2)}\\
&=\colvector{-8\\4\\-17}
%
\end{align*}
\end{para}
%
\begin{para}So the matrix representation of $T$ (relative to $B$ and $C$) is
%
\begin{equation*}
\matrixrep{T}{B}{C}
=
\begin{bmatrix}
 2 & -24 & 5 & -8 \\
 0 & 8 & 0 & 4 \\
 -2 & -26 & -5 & -17
\end{bmatrix}
\end{equation*}
\end{para}
%
\begin{para}We know from \acronymref{theorem}{KNSI} that the kernel of the linear transformation $T$ is isomorphic to the null space of the matrix representation $\matrixrep{T}{B}{C}$ and by studying the proof of \acronymref{theorem}{KNSI} we learn that $\vectrepname{B}$ is an isomorphism between these null spaces. Rather than trying to compute the kernel of $T$ using definitions and techniques from \acronymref{chapter}{LT} we will instead analyze the null space of $\matrixrep{T}{B}{C}$ using techniques from way back in \acronymref{chapter}{V}.  First row-reduce $\matrixrep{T}{B}{C}$,
%
\begin{equation*}
\begin{bmatrix}
2 & -24 & 5 & -8 \\
0 & 8 & 0 & 4 \\
-2 & -26 & -5 & -17
\end{bmatrix}
\rref
\begin{bmatrix}
 \leading{1} & 0 & \frac{5}{2} & 2 \\
 0 & \leading{1} & 0 & \frac{1}{2} \\
 0 & 0 & 0 & 0
\end{bmatrix}
\end{equation*}
\end{para}
%
\begin{para}So, by \acronymref{theorem}{BNS}, a basis for $\nsp{\matrixrep{T}{B}{C}}$ is
%
\begin{equation*}
\spn{\set{\colvector{-\frac{5}{2}\\0\\1\\0},\,\colvector{-2\\-\frac{1}{2}\\0\\1}}}
\end{equation*}
\end{para}
%
\begin{para}We can  now convert this basis of $\nsp{\matrixrep{T}{B}{C}}$ into a basis of $\krn{T}$ by applying $\vectrepinvname{B}$ to each element of the basis,
%
\begin{align*}
\vectrepinv{B}{\colvector{-\frac{5}{2}\\0\\1\\0}}&=
(-\frac{5}{2})\begin{bmatrix}1 & 2 \\ -1 & -1\end{bmatrix}+
0\begin{bmatrix}1 & 3 \\ -1 & -4\end{bmatrix}+
1\begin{bmatrix}1 & 2 \\ 0 & -2\end{bmatrix}+
0\begin{bmatrix}2 & 5 \\ -2 & -4\end{bmatrix}\\
&=\begin{bmatrix}-\frac{3}{2} & -3 \\ \frac{5}{2} & \frac{1}{2}\end{bmatrix}\\
%
\vectrepinv{B}{\colvector{-2\\-\frac{1}{2}\\0\\1}}&=
(-2)\begin{bmatrix}1 & 2 \\ -1 & -1\end{bmatrix}+
(-\frac{1}{2})\begin{bmatrix}1 & 3 \\ -1 & -4\end{bmatrix}+
0\begin{bmatrix}1 & 2 \\ 0 & -2\end{bmatrix}+
1\begin{bmatrix}2 & 5 \\ -2 & -4\end{bmatrix}\\
&=\begin{bmatrix}-\frac{1}{2} & -\frac{1}{2} \\ \frac{1}{2} &0\end{bmatrix}
\end{align*}
\end{para}
%
\begin{para}So the set
%
\begin{equation*}
\set{
\begin{bmatrix}-\frac{3}{2} & -3 \\ \frac{5}{2} & \frac{1}{2}\end{bmatrix},\,
\begin{bmatrix}-\frac{1}{2} & -\frac{1}{2} \\ \frac{1}{2} &0\end{bmatrix}
}
\end{equation*}
%
is a basis for $\krn{T}$  Just for fun, you might evaluate $T$ with each of these two basis vectors and verify that the output is the zero polynomial (\acronymref{exercise}{MR.C10}).\end{para}
%
\end{example}
%
\begin{para}An entirely similar result applies to the range of a linear transformation and the column space of a matrix representation of the linear transformation.\end{para}
%
\begin{theorem}{RCSI}{Range and Column Space Isomorphism}{range!isomorphic to column space}
\index{column space!isomorphic to range}
\begin{para}Suppose that $\ltdefn{T}{U}{V}$ is a linear transformation, $B$ is a basis for $U$ of size $n$, and $C$ is a basis for $V$ of size $m$.  Then the range of $T$ is isomorphic to the column space of $\matrixrep{T}{B}{C}$,
%
\begin{equation*}
\rng{T}\isomorphic\csp{\matrixrep{T}{B}{C}}
\end{equation*}
\end{para}
%
\end{theorem}
%
\begin{proof}
\begin{para}To establish that two vector spaces are isomorphic, we must find an isomorphism between them, an invertible linear transformation (\acronymref{definition}{IVS}).  The range of the linear transformation $T$, $\rng{T}$, is a subspace of $V$, while the column space of the matrix representation, $\csp{\matrixrep{T}{B}{C}}$ is a subspace of $\complex{m}$.  The function $\vectrepname{C}$ is defined as a function from $V$ to $\complex{m}$, but we can just as well employ the definition of $\vectrepname{C}$ as a function from $\rng{T}$ to $\csp{\matrixrep{T}{B}{C}}$.\end{para}
%
\begin{para}We must first insure that if we choose an input for $\vectrepname{C}$ from $\rng{T}$ that then the output will be an element of $\csp{\matrixrep{T}{B}{C}}$.  So suppose that $\vect{v}\in\rng{T}$.  Then there is a vector $\vect{u}\in U$, such that $\lt{T}{\vect{u}}=\vect{v}$.  Consider
%
\begin{align*}
\matrixrep{T}{B}{C}\vectrep{B}{\vect{u}}
&=\vectrep{C}{\lt{T}{\vect{u}}}&&\text{\acronymref{theorem}{FTMR}}\\
%
&=\vectrep{C}{\vect{v}}&&\text{\acronymref{definition}{RLT}}
%
\end{align*}\end{para}
%
\begin{para}This says that $\vectrep{C}{\vect{v}}\in\csp{\matrixrep{T}{B}{C}}$, as desired.\end{para}
%
\begin{para}The restriction in the size of the domain and codomain will not affect the fact that $\vectrepname{C}$ is a linear transformation (\acronymref{theorem}{VRLT}), nor will it affect the fact that $\vectrepname{C}$ is injective (\acronymref{theorem}{VRI}).  Something must be done though to verify that $\vectrepname{C}$ is surjective.  This all gets a bit confusing, since the domain of our isomorphism is the range of the linear transformation, so think about your objects as you go.  To establish that $\vectrepname{C}$ is surjective, appeal to the definition of a surjective linear transformation (\acronymref{definition}{SLT}), and suppose that we have an element of the codomain, $\vect{y}\in\csp{\matrixrep{T}{B}{C}}\subseteq\complex{m}$ and we wish to find an element of the domain with $\vect{y}$ as its image.  Since $\vect{y}\in\csp{\matrixrep{T}{B}{C}}$, there exists a vector, $\vect{x}\in\complex{n}$ with $\matrixrep{T}{B}{C}\vect{x}=\vect{y}$.\end{para}
%
\begin{para}We now show that the desired element of the domain is $\vect{v}=\vectrepinv{C}{\vect{y}}$.     First, verify that $\vect{v}\in\rng{T}$ by applying $T$ to $\vect{u}=\vectrepinv{B}{\vect{x}}$,
%
\begin{align*}
\lt{T}{\vect{u}}&=\lt{T}{\vectrepinv{B}{\vect{x}}}\\
&=\vectrepinv{C}{\matrixrep{T}{B}{C}\left(\vectrep{B}{\vectrepinv{B}{\vect{x}}}\right)}&&\text{\acronymref{theorem}{FTMR}}\\
&=\vectrepinv{C}{\matrixrep{T}{B}{C}\left(\lt{I_{\complex{n}}}{\vect{x}}\right)}&&\text{\acronymref{definition}{IVLT}}\\
&=\vectrepinv{C}{\matrixrep{T}{B}{C}\vect{x}}&&\text{\acronymref{definition}{IDLT}}\\
&=\vectrepinv{C}{\vect{y}}&&\text{\acronymref{definition}{CSM}}\\
&=\vect{v}&&\text{Substitution}\\
%
\intertext{Second, verify that the proposed isomorphism, $\vectrepname{C}$, takes $\vect{v}$ to $\vect{y}$,}
%
\vectrep{C}{\vect{v}}&=\vectrep{C}{\vectrepinv{C}{\vect{y}}}&&\text{Substitution}\\
&=\lt{I_{\complex{m}}}{\vect{y}}&&\text{\acronymref{definition}{IVLT}}\\
&=\vect{y}&&\text{\acronymref{definition}{IDLT}}
%
\end{align*}
\end{para}
%
\begin{para}With $\vectrepname{C}$ demonstrated to be an injective and surjective linear transformation from $\rng{T}$ to $\csp{\matrixrep{T}{B}{C}}$, \acronymref{theorem}{ILTIS} tells us $\vectrepname{C}$ is invertible, and so by \acronymref{definition}{IVS}, we say $\rng{T}$ and $\csp{\matrixrep{T}{B}{C}}$ are isomorphic.\end{para}
%
\end{proof}
%
\begin{example}{RVMR}{Range via matrix representation}{range!via matrix representation}
%
\begin{para}In this example, we will recycle the linear transformation $T$ and the bases $B$ and $C$ of \acronymref{example}{KVMR} but now we will compute the range of $T$,
%
\begin{equation*}
\ltdefn{T}{M_{22}}{P_2},\quad \lt{T}{\begin{bmatrix}a&b\\c&d\end{bmatrix}}=
(2a-b+c-5d)+(a+4b+5b+2d)x+(3a-2b+c-8d)x^2
\end{equation*}
\end{para}
%
\begin{para}With bases $B$ and $C$,
%
\begin{align*}
B&=\set{
\begin{bmatrix}1 & 2 \\ -1 & -1\end{bmatrix},\,
\begin{bmatrix}1 & 3 \\ -1 & -4\end{bmatrix},\,
\begin{bmatrix}1 & 2 \\ 0 & -2\end{bmatrix},\,
\begin{bmatrix}2 & 5 \\ -2 & -4\end{bmatrix}
}\\
%
C&=\set{1+x+x^2,\,2+3x,\,-1-2x^2}
\end{align*}
%
we obtain the matrix representation
%
\begin{equation*}
\matrixrep{T}{B}{C}
=
\begin{bmatrix}
 2 & -24 & 5 & -8 \\
 0 & 8 & 0 & 4 \\
 -2 & -26 & -5 & -17
\end{bmatrix}
\end{equation*}
\end{para}
%
\begin{para}We know from \acronymref{theorem}{RCSI} that the range of the linear transformation $T$ is isomorphic to the column space of the matrix representation $\matrixrep{T}{B}{C}$ and by studying the proof of \acronymref{theorem}{RCSI} we learn that $\vectrepname{C}$ is an isomorphism between these subspaces.   Notice that since the range is a subspace of the codomain, we will employ $\vectrepname{C}$ as the isomorphism, rather than $\vectrepname{B}$, which was the correct choice for an isomorphism between the null spaces of \acronymref{example}{KVMR}.\end{para}
%
\begin{para}Rather than trying to compute the range of $T$ using definitions and techniques from \acronymref{chapter}{LT} we will instead analyze the column space of $\matrixrep{T}{B}{C}$ using techniques from way back in \acronymref{chapter}{M}.  First row-reduce $\transpose{\left(\matrixrep{T}{B}{C}\right)}$,
%
\begin{equation*}
\begin{bmatrix}
2 & 0 & -2 \\
-24 & 8 & -26 \\
5 & 0 & -5 \\
-8 & 4 & -17
\end{bmatrix}
\rref
\begin{bmatrix}
\leading{1} & 0 & -1 \\
0 & \leading{1} & -\frac{25}{4} \\
0 & 0 & 0 \\
0 & 0 & 0
\end{bmatrix}
\end{equation*}
\end{para}
%
\begin{para}Now employ \acronymref{theorem}{CSRST} and \acronymref{theorem}{BRS} (there are other methods we could choose here to compute the column space, such as \acronymref{theorem}{BCS}) to obtain the basis for $\csp{\matrixrep{T}{B}{C}}$,
%
\begin{equation*}
\set{
\colvector{1\\0\\-1},\,
\colvector{0\\1\\-\frac{25}{4}}
}
\end{equation*}
\end{para}
%
\begin{para}We can  now convert this basis of $\csp{\matrixrep{T}{B}{C}}$ into a basis of $\rng{T}$ by applying $\vectrepinvname{C}$ to each element of the basis,
%
\begin{align*}
\vectrepinv{C}{\colvector{1\\0\\-1}}&=
(1+x+x^2)-(-1-2x^2)=2+x+3x^2\\
%
\vectrepinv{C}{\colvector{0\\1\\-\frac{25}{4}}}&=
(2+3x)-\frac{25}{4}(-1-2x^2)=\frac{33}{4}+3x+\frac{31}{2}x^2
\end{align*}
\end{para}
%
\begin{para}So the set
%
\begin{equation*}
\set{
2+3x+3x^2,\,
\frac{33}{4}+3x+\frac{31}{2}x^2
}
\end{equation*}
%
is a basis for $\rng{T}$.
\end{para}
%
\end{example}
%
\begin{para}\acronymref{theorem}{KNSI} and \acronymref{theorem}{RCSI} can be viewed as further formal evidence for the \miscref{principle}{Coordinatization Principle}, though they are not direct consequences.\end{para}
%
\sageadvice{LTR}{Linear Transformation Restrictions}{linear transformation!restrictions}
%
\end{subsect}
%
\begin{subsect}{IVLT}{Invertible Linear Transformations}
%
\begin{para}We have seen, both in theorems and in examples, that questions about linear transformations are often equivalent to questions about matrices.  It is the matrix representation of a linear transformation that makes this idea precise.  Here's our final theorem that solidifies this connection.\end{para}
%
\begin{theorem}{IMR}{Invertible Matrix Representations}{matrix representation!invertible}
\begin{para}Suppose that $\ltdefn{T}{U}{V}$ is a linear transformation, $B$ is a basis for $U$ and $C$ is a basis for $V$. Then $T$ is an invertible linear transformation if and only if the matrix representation of $T$ relative to $B$ and $C$, $\matrixrep{T}{B}{C}$ is an invertible matrix.  When $T$ is invertible,
%
\begin{equation*}
\matrixrep{\ltinverse{T}}{C}{B}=\inverse{\left(\matrixrep{T}{B}{C}\right)}
\end{equation*}
\end{para}
%
\end{theorem}
%
\begin{proof}
\begin{para}($\Leftarrow$)  Suppose $T$ is invertible, so the inverse linear transformation $\ltdefn{\ltinverse{T}}{V}{U}$ exists (\acronymref{definition}{IVLT}).  Both linear transformations have matrix representations relative to the bases of $U$ and $V$, namely $\matrixrep{T}{B}{C}$ and $\matrixrep{\ltinverse{T}}{C}{B}$ (\acronymref{definition}{MR}).  \end{para}
%
\begin{para}Then
%
\begin{align*}
\matrixrep{\ltinverse{T}}{C}{B}\matrixrep{T}{B}{C}
&=\matrixrep{\compose{\ltinverse{T}}{T}}{B}{B}
&&\text{\acronymref{theorem}{MRCLT}}\\
%
&=\matrixrep{I_U}{B}{B}
&&\text{\acronymref{definition}{IVLT}}\\
%
&=\left[
\left.\vectrep{B}{\lt{I_U}{\vect{u}_1}}\right|
\left.\vectrep{B}{\lt{I_U}{\vect{u}_2}}\right|
\ldots
\left|\vectrep{B}{\lt{I_U}{\vect{u}_n}}\right.
\right]
&&\text{\acronymref{definition}{MR}}\\
%
&=\left[
\left.\vectrep{B}{\vect{u}_1}\right|
\left.\vectrep{B}{\vect{u}_2}\right|
\left.\vectrep{B}{\vect{u}_3}\right|
\ldots
\left|\vectrep{B}{\vect{u}_n}\right.
\right]
&&\text{\acronymref{definition}{IDLT}}\\
%
&=\matrixcolumns{e}{n}
&&\text{\acronymref{definition}{VR}}\\
%
&=I_n
&&\text{\acronymref{definition}{IM}}
\end{align*}
\end{para}
%
\begin{para}And
\begin{align*}
\matrixrep{T}{B}{C}\matrixrep{\ltinverse{T}}{C}{B}&=
\matrixrep{\compose{T}{\ltinverse{T}}}{C}{C}
&&\text{\acronymref{theorem}{MRCLT}}\\
%
&=\matrixrep{I_V}{C}{C}
&&\text{\acronymref{definition}{IVLT}}\\
%
&=\left[
\left.\vectrep{C}{\lt{I_V}{\vect{v}_1}}\right|
\left.\vectrep{C}{\lt{I_V}{\vect{v}_2}}\right|
\ldots
\left|\vectrep{C}{\lt{I_V}{\vect{v}_n}}\right.
\right]
&&\text{\acronymref{definition}{MR}}\\
%
&=\left[
\left.\vectrep{C}{\vect{v}_1}\right|
\left.\vectrep{C}{\vect{v}_2}\right|
\left.\vectrep{C}{\vect{v}_3}\right|
\ldots
\left|\vectrep{C}{\vect{v}_n}\right.
\right]&&\text{\acronymref{definition}{IDLT}}\\
%
&=\matrixcolumns{e}{n}
&&\text{\acronymref{definition}{VR}}\\
%
&=I_n
&&\text{\acronymref{definition}{IM}}
%
\end{align*}
\end{para}
%
\begin{para}These two equations show that $\matrixrep{T}{B}{C}$ and $\matrixrep{\ltinverse{T}}{C}{B}$ are inverse matrices (\acronymref{definition}{MI}) and establish that when $T$ is invertible, then $\matrixrep{\ltinverse{T}}{C}{B}=\inverse{\left(\matrixrep{T}{B}{C}\right)}$.\end{para}
%
\begin{para}($\Leftarrow$)
Suppose now that $\matrixrep{T}{B}{C}$ is an invertible matrix and hence nonsingular (\acronymref{theorem}{NI}).  We compute the nullity of $T$,
%
\begin{align*}
\nullity{T}
&=\dimension{\krn{T}}
&&\text{\acronymref{definition}{KLT}}\\
%
&=\dimension{\nsp{\matrixrep{T}{B}{C}}}
&&\text{\acronymref{theorem}{KNSI}}\\
%
&=\nullity{\matrixrep{T}{B}{C}}
&&\text{\acronymref{definition}{NOM}}\\
%
&=0
&&\text{\acronymref{theorem}{RNNM}}
%
\end{align*}
\end{para}
%
\begin{para}So the kernel of $T$ is trivial, and by \acronymref{theorem}{KILT}, $T$ is injective.\end{para}
%
\begin{para}We now compute the rank of $T$,
%
\begin{align*}
\rank{T}
&=\dimension{\rng{T}}
&&\text{\acronymref{definition}{RLT}}\\
%
&=\dimension{\csp{\matrixrep{T}{B}{C}}}
&&\text{\acronymref{theorem}{RCSI}}\\
%
&=\rank{\matrixrep{T}{B}{C}}
&&\text{\acronymref{definition}{ROM}}\\
%
&=\dimension{V}
&&\text{\acronymref{theorem}{RNNM}}
%
\end{align*}
\end{para}
%
\begin{para}Since the dimension of the range of $T$ equals the dimension of the codomain $V$, by \acronymref{theorem}{EDYES}, $\rng{T}=V$.  Which says that $T$ is surjective by \acronymref{theorem}{RSLT}.\end{para}
%
\begin{para}Because $T$ is both injective and surjective, by \acronymref{theorem}{ILTIS}, $T$ is invertible.\end{para}
%
\end{proof}
%
\begin{para}By now, the connections between matrices and linear transformations should be starting to become more transparent, and you may have already recognized the invertibility of a matrix as being  tantamount to the invertibility of the associated matrix representation.  The next example shows how to apply this theorem to the problem of actually building a formula for the inverse of an invertible linear transformation.\end{para}
%
\begin{example}{ILTVR}{Inverse of a linear transformation via a representation}{linear transformation inverse!via matrix representation}
\begin{para}Consider the linear transformation
%
\begin{equation*}
\ltdefn{R}{P_3}{M_{22}},\quad
\lt{R}{a+bx+cx^2+x^3}=
\begin{bmatrix}
a +b - c + 2d & 2a + 3b - 2c + 3d\\
a + b + 2 d & -a + b + 2c - 5d
\end{bmatrix}
\end{equation*}
\end{para}
%
\begin{para}If we wish to quickly find a formula for the inverse of $R$ (presuming it exists), then choosing ``nice'' bases will work best.  So build a matrix representation of $R$ relative to the bases $B$ and $C$,
%
\begin{align*}
B&=\set{1,\,x,\,x^2,\,x^3}\\
C&=\set{
\begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix},\,
\begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix},\,
\begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix},\,
\begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}
}
\end{align*}
\end{para}
%
\begin{para}Then,
%
\begin{align*}
\vectrep{C}{\lt{R}{1}}
&=\vectrep{C}{\begin{bmatrix} 1 & 2 \\ 1 & -1 \end{bmatrix}}
=\colvector{1\\2\\1\\-1}\\
%
\vectrep{C}{\lt{R}{x}}
&=\vectrep{C}{\begin{bmatrix} 1 & 3 \\ 1 & 1 \end{bmatrix}}
=\colvector{1\\3\\1\\1}\\
%
\vectrep{C}{\lt{R}{x^2}}
&=\vectrep{C}{\begin{bmatrix} -1 & -2 \\ 0 & 2 \end{bmatrix}}
=\colvector{-1\\-2\\0\\2}\\
%
\vectrep{C}{\lt{R}{x^3}}
&=\vectrep{C}{\begin{bmatrix} 2 & 3 \\ 2 & -5 \end{bmatrix}}
=\colvector{2\\3\\2\\-5}
%
\end{align*}
\end{para}
%
\begin{para}So a representation of $R$ is
%
\begin{equation*}
\matrixrep{R}{B}{C}=
\begin{bmatrix}
 1 & 1 & -1 & 2 \\
 2 & 3 & -2 & 3 \\
 1 & 1 & 0 & 2 \\
 -1 & 1 & 2 & -5
\end{bmatrix}
\end{equation*}
\end{para}
%
\begin{para}The matrix $\matrixrep{R}{B}{C}$ is invertible (as you can check) so we know for sure that $R$ is invertible by \acronymref{theorem}{IMR}.  Furthermore,
%
\begin{equation*}
\matrixrep{\ltinverse{R}}{C}{B}
=\inverse{\left(\matrixrep{R}{B}{C}\right)}
=
\inverse{
\begin{bmatrix}
 1 & 1 & -1 & 2 \\
 2 & 3 & -2 & 3 \\
 1 & 1 & 0 & 2 \\
 -1 & 1 & 2 & -5
\end{bmatrix}
}
=
\begin{bmatrix}
20 & -7 & -2 & 3 \\
-8 & 3 & 1 & -1 \\
-1 & 0 & 1 & 0 \\
-6 & 2 & 1 & -1
\end{bmatrix}
\end{equation*}
\end{para}
%
\begin{para}We can use this representation of the inverse linear transformation, in concert with \acronymref{theorem}{FTMR}, to determine an explicit formula for the inverse itself,
%
\begin{align*}
\lt{\ltinverse{R}}{\begin{bmatrix}a&b\\c&d\end{bmatrix}}
&=\vectrepinv{B}{\matrixrep{\ltinverse{R}}{C}{B}\vectrep{C}{\begin{bmatrix}a&b\\c&d\end{bmatrix}}}&&\text{\acronymref{theorem}{FTMR}}\\
&=\vectrepinv{B}{\inverse{\left(\matrixrep{R}{B}{C}\right)}\vectrep{C}{\begin{bmatrix}a&b\\c&d\end{bmatrix}}}&&\text{\acronymref{theorem}{IMR}}\\
&=\vectrepinv{B}{\inverse{\left(\matrixrep{R}{B}{C}\right)}\colvector{a\\b\\c\\d}}&&\text{\acronymref{definition}{VR}}\\
&=\vectrepinv{B}{
\begin{bmatrix}
20 & -7 & -2 & 3 \\
-8 & 3 & 1 & -1 \\
-1 & 0 & 1 & 0 \\
-6 & 2 & 1 & -1
\end{bmatrix}
\colvector{a\\b\\c\\d}}&&\text{\acronymref{definition}{MI}}\\
&=\vectrepinv{B}{
\colvector{
20a - 7b - 2c + 3d\\
-8a + 3b + c -d\\
-a + c\\
-6a + 2b + c - d}}&&\text{\acronymref{definition}{MVP}}\\
&=(20a - 7b - 2c + 3d)+(-8a + 3b + c -d)x\\
&\quad\quad +(-a + c)x^2+(-6a + 2b + c - d)x^3
&&\text{\acronymref{definition}{VR}}
\end{align*}
\end{para}
%
\end{example}
%
\begin{para}You might look back at \acronymref{example}{AIVLT}, where we first witnessed the inverse of a linear transformation and recognize that the inverse ($S$) was built from using the method of \acronymref{example}{ILTVR} with a matrix representation of $T$.\end{para}
%
%
\begin{theorem}{IMILT}{Invertible Matrices, Invertible Linear Transformation}{invertible linear transformation!defined by invertible matrix}
\begin{para}Suppose that $A$ is a square matrix of size $n$ and $\ltdefn{T}{\complex{n}}{\complex{n}}$ is the linear transformation defined by $\lt{T}{\vect{x}}=A\vect{x}$.  Then $A$ is invertible matrix if and only if $T$ is an invertible linear transformation.\end{para}
\end{theorem}
%
\begin{proof}
\begin{para}Choose bases $B=C=\set{\vectorlist{e}{n}}$ consisting of the standard unit vectors as a basis of $\complex{n}$ (\acronymref{theorem}{SUVB}) and build a matrix representation of $T$ relative to $B$ and $C$.  Then
%
\begin{align*}
\vectrep{C}{\lt{T}{\vect{e}_i}}
&=\vectrep{C}{A\vect{e}_i}\\
%
&=\vectrep{C}{\vect{A}_i}\\
%
&=\vect{A}_i
\end{align*}
\end{para}
%
\begin{para}So then the matrix representation of $T$, relative to $B$ and $C$, is simply $\matrixrep{T}{B}{C}=A$.  with this observation, the proof becomes a specialization of \acronymref{theorem}{IMR},
%
\begin{align*}
%
T\text{ is invertible}
\iff\matrixrep{T}{B}{C}\text{ is invertible}
\iff A\text{ is invertible}
\end{align*}
\end{para}
%
\end{proof}
%
\begin{para}This theorem may seem gratuitous.  Why state such a special case of \acronymref{theorem}{IMR}?  Because it adds another condition to our NMEx series of theorems, and in some ways it is the most fundamental expression of what it means for a matrix to be nonsingular --- the associated linear transformation is invertible.  This is our final update.\end{para}
%
\begin{theorem}{NME9}{Nonsingular Matrix Equivalences, Round 9}{nonsingular matrix!equivalences}
\begin{para}Suppose that $A$ is a square matrix of size $n$.  The following are equivalent.
%
\begin{enumerate}
\item $A$ is nonsingular.
\item $A$ row-reduces to the identity matrix.
\item The null space of $A$ contains only the zero vector, $\nsp{A}=\set{\zerovector}$.
\item The linear system $\linearsystem{A}{\vect{b}}$ has a unique solution for every possible choice of $\vect{b}$.
\item The columns of $A$ are a linearly independent set.
\item $A$ is invertible.
\item The column space of $A$ is $\complex{n}$, $\csp{A}=\complex{n}$.
\item The columns of $A$ are a basis for $\complex{n}$.
\item The rank of $A$ is $n$, $\rank{A}=n$.
\item The nullity of $A$ is zero, $\nullity{A}=0$.
\item The determinant of $A$ is nonzero, $\detname{A}\neq 0$.
\item $\lambda=0$ is not an eigenvalue of $A$.
\item The linear transformation $\ltdefn{T}{\complex{n}}{\complex{n}}$ defined by $\lt{T}{\vect{x}}=A\vect{x}$ is invertible.
\end{enumerate}
\end{para}
\end{theorem}
%
\begin{proof}
\begin{para}By \acronymref{theorem}{IMILT} the new addition to this list is equivalent to the statement that $A$ is invertible so we can expand \acronymref{theorem}{NME8}.\end{para}
\end{proof}
%
\sageadvice{NME9}{Nonsingular Matrix Equivalences, Round 9}{nonsingular matrix equivalences, round 9}
%
\end{subsect}
%
%  End of  mr.tex