%%%%(c)
%%%%(c)  This file is a portion of the source for the textbook
%%%%(c)
%%%%(c)    A First Course in Linear Algebra
%%%%(c)    Copyright 2004 by Robert A. Beezer
%%%%(c)
%%%%(c)  See the file COPYING.txt for copying conditions
%%%%(c)
%%%%(c)
%%%%%%%%%%%
%%
%%  Annotated Acronyms R
%%  Representations
%%
%%%%%%%%%%%
%
\annoacro{definition}{VR}{%
Matrix representations build on vector representations, so this is the definition that gets us started.  A representation depends on the choice of a single basis for the vector space.  \acronymref{theorem}{VRRB} is what tells us this idea might be useful.
}
%
\annoacro{theorem}{VRILT}{%
As an invertible linear transformation, vector representation allows us to translate, back and forth, between abstract vector spaces ($V$) and concrete vector spaces ($\complex{n}$).  This is key to all our notions of representations in this chapter.
}
%
\annoacro{theorem}{CFDVS}{%
Every vector space with finite dimension ``looks like'' a vector space of column vectors.  Vector representation is the isomorphism that establishes that these vector spaces are isomorphic.
}
%
\annoacro{definition}{MR}{%
Building on the definition of a vector representation, we define a representation of a linear transformation, determined by a choice of two bases, one for the domain and one for the codomain.  Notice that vectors are represented by columnar lists of scalars, while linear transformations are represented by rectangular tables of scalars.  Building a matrix representation is as important a skill as row-reducing a matrix.
}
%
\annoacro{theorem}{FTMR}{%
\acronymref{definition}{MR} is not really very interesting until we have this theorem.  The second form tells us that we can compute outputs of linear transformations via matrix multiplication, along with some bookkeeping for vector representations.  Searching forward through the text on ``FTMR'' is an interesting exercise.  You will find reference to this result buried inside many key proofs at critical points, and it also appears in numerous examples and solutions to exercises.
}
%
\annoacro{theorem}{MRCLT}{%
Turns out that matrix multiplication is really a very natural operation, it is just the chaining together (composition) of functions (linear transformations).  Beautiful.  Even if you don't try to work the problem, study \acronymref{solution}{MR.T80} for more insight.
}
%
\annoacro{theorem}{KNSI}{%
Kernels ``are'' null spaces.  For this reason you'll see these terms used interchangeably.
}
%
\annoacro{theorem}{RCSI}{%
Ranges ``are'' column spaces.  For this reason you'll see these terms used interchangeably.
}
%
\annoacro{theorem}{IMR}{%
Invertible linear transformations are represented by invertible (nonsingular) matrices.
}
%
\annoacro{theorem}{NME9}{%
The NMEx series has always been important, but we've held off saying so until now.  This is the end of the line for this one, so it is a good time to contemplate all that it means.
}
%
\annoacro{theorem}{SCB}{%
Diagonalization back in \acronymref{section}{SD} was really a change of basis to achieve a diagonal matrix repesentation.  Maybe we should be highlighting the more general \acronymref{theorem}{MRCB} here, but its overly technical description just isn't as appealing.  However, it will be important in some of the matrix decompostions in \acronymref{chapter}{MD}.
}
%
\annoacro{theorem}{EER}{%
This theorem, with the companion definition, \acronymref{definition}{EELT}, tells us that eigenvalues, and eigenvectors, are fundamentally a characteristic of linear transformations (not matrices).  If you study matrix decompositions in \acronymref{chapter}{MD} you will come to appreciate that almost all of a matrix's secrets can be unlocked with knowledge of the eigenvalues and eigenvectors.
}
%
\annoacro{theorem}{OD}{%
Can you imagine anything nicer than an orthonormal diagonalization?  A basis of pairwise orthogonal, unit norm, eigenvectors that provide a diagonal representation for a matrix?  Here we learn just when this can happen --- precisely when a matrix is normal, which is a disarmingly simple property to define.
}
%
\annoacro{theorem}{CFNLT}{%
Nilpotent linear transformations are the fundamental obstacle to a matrix (or linear transformation) being diagonalizable.  This specialized representation theorem is the fundamental expression of just how close we can come to surmounting the obstacle, i.e.\ how close we can come to a diagonal representation.
}
%
\annoacro{theorem}{DGES}{%
This theorem is a long time in coming, but perhaps it best explains our interest in generalized eigenspaces.  When the dimension of a ``regular'' eigenspace (the geometic multiplicity) does not meet the algebraic multiplicity of the corresponding eigenvalue, then a matrix is not diagonalizable (\acronymref{theorem}{DMFE}).  However, if we generalize the idea of an eigenspace (\acronymref{definition}{GES}), then we arrive at invariant subspaces that together give a complete decomposition of the domain as a direct sum.  And these subspaces have dimensions equal to the corresponding algebraic multiplicities.
}
%
\annoacro{theorem}{JCFLT}{%
If you can't diagonalize, just how close can you come?  This is an answer (there are others, like rational canonical form).  ``Canonicalism'' is in the eye of the beholder.  But this is a good place to conclude our study of a widely accepted canonical form that is possible for every matrix or linear transformation.
}
%
% End R.tex annotated acronyms
