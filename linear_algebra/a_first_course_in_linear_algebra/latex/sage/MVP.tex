A matrix-vector product is very natural in Sage, and we can check the result against a linear combination of the columns.
%
\begin{sageexample}
sage: A = matrix(QQ, [[1, -3,  4,  5],
...                   [2,  3, -2,  0],
...                   [5,  6,  8, -2]])
sage: v = vector(QQ, [2, -2, 1, 3])
sage: A*v
(27, -4, 0)
sage: sum([v[i]*A.column(i) for i in range(len(v))])
(27, -4, 0)
\end{sageexample}
%
Notice that when a matrix is square, a vector of the correct size can be used in Sage in a product with a matrix by placing the vector on either side of the matrix.  However, the two results are \emph{not} the same, and we will not have ocassion to place the vector on the left any time soon.  So, despite the possibility, be certain to keep your vectors on the right side of a matrix in a product.
%
\begin{sageexample}
sage: B = matrix(QQ, [[ 1, -3,  4,  5],
...                   [ 2,  3, -2,  0],
...                   [ 5,  6,  8, -2],
...                   [-4,  1,  1,  2]])
sage: w = vector(QQ, [1, 2, -3, 2])
sage: B*w
(-7, 14, -11, -1)
sage: w*B
(-18, -13, -22, 15)
sage: B*w == w*B
False
\end{sageexample}
%
Since a matrix-vector product forms a linear combination of columns of a matrix, it is now very easy to check if a vector is a solution to a system of equations.  This is basically the substance of \acronymref{theorem}{SLEMM}.  Here we construct a system of equations and construct two solutions and one non-solution by applying \acronymref{theorem}{PSPHS}.  Then we use a matrix-vector product to verify that the vectors are, or are not, solutions.
%
\begin{sageexample}
sage: coeff = matrix(QQ, [[-1,  3, -1, -1,  0,  2],
...                       [ 2, -6,  1, -2, -5, -8],
...                       [ 1, -3,  2,  5,  4,  1],
...                       [ 2, -6,  2,  2,  1, -3]])
sage: const = vector(QQ, [13, -25, -17, -23])
sage: solution1 = coeff.solve_right(const)
sage: coeff*solution1
(13, -25, -17, -23)
sage: nsp = coeff.right_kernel(basis='pivot')
sage: nsp
Vector space of degree 6 and dimension 3 over Rational Field
User basis matrix:
[ 3  1  0  0  0  0]
[ 3  0 -4  1  0  0]
[ 1  0  1  0 -1  1]
sage: nspb = nsp.basis()
sage: solution2 = solution1 + 5*nspb[0]+(-4)*nspb[1]+2*nspb[2]
sage: coeff*solution2
(13, -25, -17, -23)
sage: nonnullspace = vector(QQ, [5, 0, 0, 0, 0, 0])
sage: nonnullspace in nsp
False
sage: nonsolution = solution1 + nonnullspace
sage: coeff*nonsolution
(8, -15, -12, -13)
\end{sageexample}
%
We can now explain the difference between ``left'' and ``right'' variants of various Sage commands for linear algebra.  Generally, the direction refers to \emph{where the vector is placed} in a matrix-vector product.  We place a vector on the right and understand this to mean a linear combination of the columns of the matrix.  Placing a vector to the left of a matrix can be understood, in a manner totally consistent with our upcoming definition of matrix multiplication, as a linear combination of the \emph{rows} of the matrix.\par
%
So the difference between \verb?A.solve_right(v)? and \verb?A.solve_left(v)? is that the former asks for a vector \verb?x? such that \texttt{A*x == v}, while the latter asks for a vector \verb?x? such that \texttt{x*A == v}.  Given Sage's preference for rows, a direction-neutral version of a command, if it exists, will be the ``left'' version.  For example, there is a \verb?.right_kernel()? matrix method, while the \verb?.left_kernel()? and \verb?.kernel()? methods are identical --- the names are synonyms for the exact same routine.\par
%
So when you see a Sage command that comes in ``left'' and ``right'' variants figure out just what part of the defined object involves a matrix-vector product and form an interpretation from that.
%
\begin{sageverbatim}
\end{sageverbatim}
%

